{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workinprogress.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygoosh/samsung/blob/master/workinprogress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_Mg8x0gASeS",
        "colab_type": "code",
        "outputId": "b4b278ec-2da4-4ece-f09c-923c25602f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from google.colab import files, drive\n",
        "from collections import defaultdict\n",
        "\n",
        "import re\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array, argmax, random, take\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, GRU, Embedding, RepeatVector, TimeDistributed\n",
        "from keras.layers import Bidirectional as Bi\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "RS = 77\n",
        "rn.seed(RS)\n",
        "# tf.random.set_seed(RS)\n",
        "np.random.seed(RS)\n",
        "np.random.RandomState(RS)\n",
        "\n",
        "gpath = Path('/content/gdrive')\n",
        "drive.mount(str(gpath))\n",
        "data_file = gpath / 'My Drive/Samsung' / 'transcriptions'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a0fc22d70461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mgpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'My Drive/Samsung'\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'transcriptions'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KByWPn1DbpS",
        "colab_type": "text"
      },
      "source": [
        "#### Let's look at the data in given file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNLzpBAGDgAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with data_file.open() as f:  \n",
        "    print(list(f.readline()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmvmq8NkDzzF",
        "colab_type": "text"
      },
      "source": [
        "#### Notice that:\n",
        "1. russian sentence is separated from transcript with '\\t'\n",
        "2. the begining and the end of transcript part are marked by '%%'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9inDldeBBz8",
        "colab_type": "text"
      },
      "source": [
        "### Let's read the data and split it into rus and trans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze9lEBcZB4A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn = []\n",
        "with data_file.open() as f:  \n",
        "  for line in f: \n",
        "    rus, trn = line.split('\\t')\n",
        "    rus_trn.append([rus.strip(), trn.strip()])\n",
        "\n",
        "print(f'Number of sentences in corpus: {len(rus_trn)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjFV-XOHBN8o",
        "colab_type": "text"
      },
      "source": [
        "##### Let's look at some sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaCdAWXfBS-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = np.random.choice(len(rus_trn), 3)\n",
        "\n",
        "print(rus_trn[a][0])\n",
        "print(rus_trn[a][1])\n",
        "print(rus_trn[b][0])\n",
        "print(rus_trn[b][1])\n",
        "print(rus_trn[c][0])\n",
        "print(rus_trn[c][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrtVAlRiBYLc",
        "colab_type": "text"
      },
      "source": [
        "#### Notice that words in russian sentence are separated by space while words in transcript are separated:\n",
        "1. by '#' in general case\n",
        "2. by '_' in case of preposition\n",
        "3. by '%% %%' in case of punctuation signs (dash, coma, etc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYfq4rSFVs6",
        "colab_type": "text"
      },
      "source": [
        "#### Let's see if we have dupliates in corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoPm3mehFfiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = array(rus_trn)\n",
        "seen_rus = Counter(a[:,0])\n",
        "seen_trn = Counter(a[:,1])\n",
        "\n",
        "print(f'Unique rus sentences: {len(seen_rus)} out of {len(rus_trn)}')\n",
        "print(f'Unique trans sentences: {len(seen_trn)} out of {len(rus_trn)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c_7KSzLGs7v",
        "colab_type": "text"
      },
      "source": [
        "#### We have a lot of duplicates! Only 3131 unique sentenses out of 50K in corpus. Please also note that some russian sentences are transcribed into different transcriptions (will look into that later on)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjR7uun4HeJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lens = [each[1] for each in seen_rus.items()]\n",
        "unq = np.unique(lens)\n",
        "qty = [lens.count(each) for each in unq]\n",
        "pd.DataFrame(qty, index=unq).plot.bar(title = 'Rus sentence repeat times', legend=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2bIKlVbIAxi",
        "colab_type": "text"
      },
      "source": [
        "#### Most duplicated sentences repeat 12 times, max up to 203 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCi44KJRImmG",
        "colab_type": "text"
      },
      "source": [
        "### Let's read that data while splitting the tokens. We'll count the tokens in each sentence. If the count in rus and trans is different, we'll record it as anomaly\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fznr-0SJ9nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_l, trn_l, anomaly = [], [], []\n",
        "seen = defaultdict(list)\n",
        "chars = Counter()\n",
        "\n",
        "for j,i in enumerate(rus_trn):\n",
        "  rus = i[0].split()\n",
        "  trn = re.split('#|_|%% %%',i[1])\n",
        "\n",
        "  if i[0] not in seen:\n",
        "    if abs(len(rus) - len(trn)) != 0: \n",
        "      anomaly.append(j)\n",
        "      # print(j, ' ', i[0])\n",
        "      # print( i[1])\n",
        "    \n",
        "    else:\n",
        "      rus_l.append(len(rus))\n",
        "      trn_l.append(len(trn))\n",
        "\n",
        "    chars += Counter(i[0])\n",
        "\n",
        "  seen[i[0]].append(j)\n",
        "\n",
        "print(f'Anomalies: {len(anomaly)}')\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "pd.DataFrame({'Number of words in Rus sentence':rus_l}).hist(ax=ax1, bins = 30);\n",
        "pd.DataFrame({'Number of words in Trns sentence':trn_l}).hist(ax=ax2, bins = 30);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxTcio_yKSbh",
        "colab_type": "text"
      },
      "source": [
        "#### So we have 49 anomalies out of 3131 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_cJH70KuSg",
        "colab_type": "text"
      },
      "source": [
        "### Let's see if we need to clean the data. First let's take a look at rus corpus alphabet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyV9anvZn187",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = sorted(chars.items())\n",
        "pd.DataFrame(s, index=(e[0]+' ' for e in s)).plot.bar(figsize=(18,4), rot=0, title = 'Char frequencies', legend=False)\n",
        "print(f'Number of times \"-\" used: {chars[\"-\"]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TZ18z9K8Qy",
        "colab_type": "text"
      },
      "source": [
        "#### Looks good! We neither have punctuations nor capital letters. The only case to check is '-' letter which is used 159 times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqkK_LRKLb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seen = set()\n",
        "seen_dash = set()\n",
        "dash_words = []\n",
        "for rus,trn in rus_trn:\n",
        "  if rus not in seen:\n",
        "    if '-' in rus:\n",
        "      ru_words = rus.split()\n",
        "      for each in ru_words:\n",
        "        if '-' in each:\n",
        "          if each not in seen_dash:\n",
        "            seen_dash.add(each)\n",
        "            dash_words.append(each)\n",
        "  else:\n",
        "    seen.add(rus)\n",
        "  \n",
        "print(dash_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VHh6JUCPGlb",
        "colab_type": "text"
      },
      "source": [
        "#### Okey, words with dash look fine, we'll consider dash as a normal letter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xAjHqyjh3x",
        "colab_type": "text"
      },
      "source": [
        "### So in order to implement autocoder for transcript we'd need to keep special symbools in transcript such as \"\\_\" and \"%% %%\". So let's create a dictionary of sentences rus <-> trans. So let's recreate the dictionary so \"\\_\" and \"%% %%\" are marked with '#' and remove begin and end markers. Plus to that let's get rid of duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7827cG05JrMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install transliterate\n",
        "# from transliterate import translit\n",
        "# translit('длавды дылпадыал лыдап', 'ru', reversed=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBgF1l93y39m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seen_rus = set()\n",
        "seen_trn = set()\n",
        "\n",
        "res, trn_l, rus_l = [], [], []\n",
        "for rus, trn in rus_trn:\n",
        "\n",
        "  if rus not in seen_rus or trn not in seen_trn:\n",
        "      seen_rus.add(rus)\n",
        "      seen_trn.add(trn)\n",
        "\n",
        "      # trn = trn.replace('%% %%', '%% %% #').replace('_', '_ #')[3:-3]\n",
        "      # rus_l.append( len(rus))\n",
        "      # trn_l.append( len(trn.split(' ')))\n",
        "      # res.append([rus, trn.split(' ')])\n",
        "\n",
        "      trn = trn.replace('%% %%', '%% %% #').replace('_', '_ #')[2:-2]\n",
        "      rus_l.append( len(rus.split()))\n",
        "      trn_l.append( len(trn.split('#')))\n",
        "      # res.append([rus.split(), ['<sos>'] + trn.split('#'), ['<sos>'] + trn.split('#')])\n",
        "      res.append([rus.split(), trn.split('#')])\n",
        "\n",
        "rus_trn_new = array(res)\n",
        "trn_length = max(trn_l)\n",
        "rus_length = max(rus_l)\n",
        "print(f'max len rus: {rus_length}, max len trn: {trn_length}')\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "pd.DataFrame({'Number of words in Rus sentence':rus_l}).hist(ax=ax1, bins = 30);\n",
        "pd.DataFrame({'Number of words in Trns sentence':trn_l}).hist(ax=ax2, bins = 30);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID8a8MoaKtVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn_new[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQ82gWtkTDx",
        "colab_type": "text"
      },
      "source": [
        "### Text to Sequence Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt-GGIUNlkCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenization(lines, split=' ', char_level=False):\n",
        "  tokenizer = Tokenizer(filters='', lower=False, split=split, char_level=char_level)\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  seq = pad_sequences(sequences=seq, maxlen=length, padding='post')\n",
        "  return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "670W8CC1llqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_tokenizer = tokenization(rus_trn_new[:, 0], split='',  char_level=True)\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
        "trn_tokenizer = tokenization(rus_trn_new[:, 1], split=' ', char_level=False)\n",
        "trn_vocab_size = len(trn_tokenizer.word_index) + 1\n",
        "print(f'Rus Vocabulary Size: {rus_vocab_size}')\n",
        "print(f'Trns Vocabulary Size: {trn_vocab_size}')\n",
        "a = np.random.choice(rus_vocab_size)\n",
        "print(trn_tokenizer.index_word[a])\n",
        "print(rus_tokenizer.index_word[a])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJdhdjvqRA9",
        "colab_type": "text"
      },
      "source": [
        "### Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaO755OI17PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn_new.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XcT-AUGqa5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(rus_trn_new, test_size=0.01, random_state = RS)\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(rus_tokenizer, rus_length, train[:, 0])\n",
        "# trainX_ = encode_sequences(trn_tokenizer, trn_length, train[:, 2])\n",
        "trainY = encode_sequences(trn_tokenizer, trn_length, train[:, 1])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(rus_tokenizer, rus_length, test[:, 0])\n",
        "# testX_ = encode_sequences(trn_tokenizer, trn_length, test[:, 2])\n",
        "testY = encode_sequences(trn_tokenizer, trn_length, test[:, 1])\n",
        "\n",
        "trainX.shape, testX.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGzWdS9cips_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(f'{t} ----> {lang.index_word[t]}')\n",
        "\n",
        "a = np.random.choice(len(trainX))\n",
        "convert(rus_tokenizer, trainX[a])\n",
        "print ()\n",
        "convert(trn_tokenizer, trainY[a])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5FSiYilVXvv",
        "colab_type": "text"
      },
      "source": [
        "#### FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8eeBkKPCPse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "rus_model = FastText(size=20)\n",
        "trn_model = FastText(size=20)\n",
        "\n",
        "rus_model.build_vocab(sentences=rus_trn_new[:, 0])\n",
        "trn_model.build_vocab(sentences=rus_trn_new[:, 1])\n",
        "\n",
        "total_rus = rus_model.corpus_total_words\n",
        "total_trn = trn_model.corpus_total_words\n",
        "print(total_rus, total_trn)\n",
        "\n",
        "rus_model.train(sentences=rus_trn_new[:,0], total_examples = rus_model.corpus_count, epochs=rus_model.iter)\n",
        "trn_model.train(sentences=rus_trn_new[:,1], total_examples = trn_model.corpus_count, epochs=trn_model.iter)\n",
        "\n",
        "print(rus_model.wv['и'])\n",
        "print(trn_model.wv[\" i _ \"])\n",
        "\n",
        "rus_model.wv.similar_by_vector(trn_model.wv[\" i _\"])\n",
        "# trn_model.wv.similar_by_vector(rus_model.wv['группа'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EovHsdWV0JJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_model.most_similar(' d' 'ix p u t a1 t ')\n",
        "# rus_model.most_similar('депутат')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8zTiADVilf",
        "colab_type": "text"
      },
      "source": [
        "### Moving on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a9-aKMvp3OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY[17]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwOy43R15_yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BS = 128\n",
        "EPOCHES = 500\n",
        "\n",
        "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  input = x = Input(shape=(None,))\n",
        "  x = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)(x)\n",
        "  x, h, c = LSTM(units, return_state=True)(x)\n",
        "  x = RepeatVector(out_timesteps)(x)\n",
        "  x = LSTM(units, return_sequences=True)(x, initial_state = (h, c))\n",
        "  output = TimeDistributed(Dense(out_vocab, activation='softmax'))(x)\n",
        "  inference_model = model = Model(input, output)\n",
        "  return model, inference_model\n",
        "\n",
        "def define_model1(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "   # encoder\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  encoder_embedding = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)\n",
        "  encoder = encoder_embedding(encoder_inputs)\n",
        "  _, state_h, state_c = LSTM(units, return_state=True)(encoder)\n",
        "  encoder_states = [state_h, state_c]\n",
        "  # decoder\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  decoder_embedding = Embedding(out_vocab, units, input_length=out_timesteps, mask_zero=True)\n",
        "  decoder = decoder_embedding(decoder_inputs)\n",
        "  decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
        "  decoder_dense = Dense(out_vocab, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  #inference model\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "  decoder_state_input_h = Input(shape=(units,))\n",
        "  decoder_state_input_c = Input(shape=(units,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_inputs_single = Input(shape=(None,))\n",
        "  decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "  decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
        "  decoder_states = [h, c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  inference_model = Model(\n",
        "      [decoder_inputs_single] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states\n",
        "  )\n",
        "  return model, inference_model\n",
        "\n",
        "model, model_inf = define_model(rus_vocab_size, trn_vocab_size, rus_length, trn_length, 128)\n",
        "model.summary()\n",
        "\n",
        "# optimizer = optimizers.RMSprop(lr=0.001)\n",
        "optimizer=optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "monitor = 'val_loss'\n",
        "mode = 'min'\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor=monitor, verbose=1, save_best_only=True, mode=mode)\n",
        "early_stop = EarlyStopping( patience=5, monitor=monitor, mode=mode)\n",
        "\n",
        "# train model\n",
        "history = model.fit(trainX, trainY.reshape(*trainY.shape, 1),\n",
        "# history = model.fit([trainX,trainY], trainY.reshape(*trainY.shape, 1),\n",
        "                    validation_data=(testX, testY.reshape(*testY.shape, 1)),\n",
        "                    # validation_data=([testX,trainY], testY.reshape(*testY.shape, 1)),\n",
        "                    epochs=EPOCHES, batch_size=BS, callbacks=[], \n",
        "                    verbose=1)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W4L0r-6Cwwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = load_model('model.h5')\n",
        "preds = model_inf.predict(testX)\n",
        "preds1 = np.argmax(preds,axis=2)\n",
        "a = np.random.choice(testY.shape[0])\n",
        "preds[a], testY[a]\n",
        "convert(trn_tokenizer, preds1[a])\n",
        "print()\n",
        "convert(rus_tokenizer, testX[a])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nTzqkY5ZtGdS",
        "colab": {}
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == n:\n",
        "          return word\n",
        "  return None\n",
        "  \n",
        "a = np.random.choice(trn_vocab_size)\n",
        "t = []\n",
        "for e in testX[0]:\n",
        "  s = get_word(e, rus_tokenizer)\n",
        "  if s: t.append(s)\n",
        "print( ' '.join(t) )\n",
        "\n",
        "t = []\n",
        "for e in testY[0]:\n",
        "  s = get_word(e, trn_tokenizer)\n",
        "  if s: t.append(s)\n",
        "print( '#'.join(t) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsT8avD8C-wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_text = []\n",
        "prd = preds[:3]\n",
        "for i in prd:\n",
        "  temp = []\n",
        "  for j in range(len(i)):\n",
        "    t = get_word(i[j], trn_tokenizer)\n",
        "    if j > 0:\n",
        "      if (t == get_word(i[j-1], trn_tokenizer)) or (t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t+'#')\n",
        "    else:\n",
        "      if(t == None):\n",
        "            temp.append('')\n",
        "      else:\n",
        "              temp.append(t+'#') \n",
        "\n",
        "  preds_text.append(''.join(temp))\n",
        "\n",
        "def unpack(tensor):\n",
        "  t = []\n",
        "  for e in tensor:\n",
        "    s = get_word(e, rus_tokenizer)\n",
        "    if s: t.append(s)\n",
        "  return ' '.join(t)\n",
        "\n",
        "preds_text\n",
        "# print( list(zip((unpack(e) for e in testX[:3]), preds_text)))\n",
        "# pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "# pred_df.sample(15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVg8UxovdB8",
        "colab_type": "text"
      },
      "source": [
        "# German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgbiEv2w_W_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IPzaHenLOFc",
        "colab_type": "code",
        "outputId": "5e0b23af-c8ed-4244-913a-d36c5faf23ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "  \n",
        "# split text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents\n",
        "\n",
        "# download data from http://www.manythings.org/anki/deu-eng.zip\n",
        "data = read_text(\"deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)\n",
        "\n",
        "# use first 50,000 English-German sentence pairs\n",
        "deu_eng = deu_eng[:50000,:]\n",
        "\n",
        "# Text Pre-processing\n",
        "# Remove punctuation\n",
        "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
        "deu_eng[:,2] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        " \n",
        "# convert to lowercase\n",
        "for i in range(len(deu_eng)):\n",
        "    deu_eng[i,1] = deu_eng[i,1].lower() # input (DE)\n",
        "    deu_eng[i,0] = deu_eng[i,0].lower() # target output (EN)\n",
        "    \n",
        "    deu_eng[i,2] = '<sos> ' + deu_eng[i,0] # target input (EN)\n",
        "    deu_eng[i,0] = deu_eng[i,0] + ' <eos>'# target output (EN)\n",
        "\n",
        "# Convert text to sequence \n",
        "# empty lists\n",
        "eng_l = []\n",
        "deu_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "    eng_l.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "    deu_l.append(len(i.split()))\n",
        "\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "  \n",
        "# prepare english tokenizer\n",
        "# deu_eng[0,2] += ' <eos>'\n",
        "eng_tokenizer = tokenization(deu_eng[:, 2])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "\n",
        "# prepare Deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines, sos=False, eos=False):\n",
        "    # integer encode sequences\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq\n",
        "  \n",
        "# model building\n",
        "# split data \n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "trainX_ = encode_sequences(eng_tokenizer, eng_length, train[:, 2])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
        "testX_ = encode_sequences(eng_tokenizer, eng_length, test[:, 2])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 6346\n",
            "Deutch Vocabulary Size: 10501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ4mYVSezSpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "1a6a1cca-0b04-4d51-87d9-3f650c56f945"
      },
      "source": [
        "deu_eng[:, 0], deu_eng[:, 1], deu_eng[:, 2]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['hi <eos>', 'hi <eos>', 'run <eos>', ...,\n",
              "        'he has no specific aim <eos>', 'he has only four pesos <eos>',\n",
              "        'he has stopped smoking <eos>'], dtype='<U537'),\n",
              " array(['hallo', 'grüß gott', 'lauf', ..., 'er hat kein bestimmtes ziel',\n",
              "        'er hat nur vier pesos', 'er hörte mit dem rauchen auf'],\n",
              "       dtype='<U537'),\n",
              " array(['<sos> hi', '<sos> hi', '<sos> run', ...,\n",
              "        '<sos> he has no specific aim', '<sos> he has only four pesos',\n",
              "        '<sos> he has stopped smoking'], dtype='<U537'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0TnfxYFST65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4aa95f8f-28f7-4d8d-bf71-2fac6a3f7490"
      },
      "source": [
        "trainX[0], trainX_[0], trainY[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 53,   3, 325, 487,   0,   0,   0,   0], dtype=int32),\n",
              " array([  1,  68,   5,  22, 437,   0,   0,   0], dtype=int32),\n",
              " array([  68,    5,   22,  437, 4301,    0,    0,    0], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3a6K0uQ-m4K",
        "colab_type": "code",
        "outputId": "6bd6e303-8e1d-459c-d5f7-048b915f968a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHES = 100\n",
        "BS = 512\n",
        "\n",
        "TF = False\n",
        "\n",
        "# build NMT model\n",
        "def define_model_simple(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  input = x = Input(shape=(None,))\n",
        "  x = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)(x)\n",
        "  x, h, c = LSTM(units, return_state=True)(x)\n",
        "  x = RepeatVector(out_timesteps)(x)\n",
        "  x = LSTM(units, return_sequences=True)(x, initial_state = (h, c))\n",
        "  output = TimeDistributed(Dense(out_vocab, activation='softmax'))(x)\n",
        "  model = Model(input, output)\n",
        "  return model, None, None\n",
        "\n",
        "def define_model_tf(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "   # encoder\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  encoder_embedding = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)\n",
        "  encoder = encoder_embedding(encoder_inputs)\n",
        "  _, state_h, state_c = LSTM(units, return_state=True)(encoder)\n",
        "  encoder_states = [state_h, state_c]\n",
        "  # decoder\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  decoder_embedding = Embedding(out_vocab, units, input_length=out_timesteps, mask_zero=True)\n",
        "  decoder = decoder_embedding(decoder_inputs)\n",
        "  decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
        "  decoder_dense = Dense(out_vocab, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  #inference model\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "  decoder_state_input_h = Input(shape=(units,))\n",
        "  decoder_state_input_c = Input(shape=(units,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_outputs, h, c = decoder_lstm(decoder, initial_state=decoder_states_inputs)\n",
        "  decoder_states = [h, c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = Model(\n",
        "      [decoder_inputs] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states)\n",
        "  \n",
        "  return model, encoder_model, decoder_model\n",
        "\n",
        "define_model = define_model_tf if TF else define_model_simple\n",
        "\n",
        "model, encoder_model, decoder_model = define_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "model.summary()\n",
        "  \n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "monitor = 'val_loss'\n",
        "mode = 'min'\n",
        "early_stop = EarlyStopping( patience=5, monitor=monitor, mode=mode, restore_best_weights=True)\n",
        "\n",
        "if TF:\n",
        "  data = [trainX, trainX_], trainY.reshape(*trainY.shape, 1)\n",
        "  validation_data = [testX, testX_], testY.reshape(*testY.shape, 1)\n",
        "else:\n",
        "  data = trainX, trainY.reshape(*trainY.shape, 1)\n",
        "  validation_data = testX, testY.reshape(*testY.shape, 1)\n",
        "\n",
        "history = model.fit(*data, \n",
        "          epochs=EPOCHES, batch_size=BS, \n",
        "          validation_data=validation_data,\n",
        "          callbacks=[early_stop], verbose=1)\n",
        "\n",
        "# plot validation loss vs training loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_19 (InputLayer)           (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_11 (Embedding)        (None, 8, 512)       5376512     input_19[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  [(None, 512), (None, 2099200     embedding_11[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_3 (RepeatVector)  (None, 8, 512)       0           lstm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_14 (LSTM)                  (None, 8, 512)       2099200     repeat_vector_3[0][0]            \n",
            "                                                                 lstm_13[0][1]                    \n",
            "                                                                 lstm_13[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_3 (TimeDistrib (None, 8, 6346)      3255498     lstm_14[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 12,830,410\n",
            "Trainable params: 12,830,410\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 13s 330us/step - loss: 3.7910 - val_loss: 2.9055\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 9s 220us/step - loss: 2.8301 - val_loss: 2.8050\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 9s 220us/step - loss: 2.7100 - val_loss: 2.6585\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 9s 219us/step - loss: 2.4905 - val_loss: 2.4518\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 9s 222us/step - loss: 2.2851 - val_loss: 2.2865\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 9s 222us/step - loss: 2.0947 - val_loss: 2.1290\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 9s 226us/step - loss: 1.9075 - val_loss: 1.9955\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 9s 225us/step - loss: 1.7358 - val_loss: 1.8574\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 9s 225us/step - loss: 1.5767 - val_loss: 1.7370\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 9s 223us/step - loss: 1.4265 - val_loss: 1.6487\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 9s 222us/step - loss: 1.2888 - val_loss: 1.5684\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 9s 222us/step - loss: 1.1633 - val_loss: 1.4580\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 9s 221us/step - loss: 1.0405 - val_loss: 1.3861\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 9s 222us/step - loss: 0.9351 - val_loss: 1.3243\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 9s 223us/step - loss: 0.8445 - val_loss: 1.2797\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 9s 217us/step - loss: 0.7645 - val_loss: 1.2454\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 9s 218us/step - loss: 0.6954 - val_loss: 1.2078\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 9s 220us/step - loss: 0.6367 - val_loss: 1.1837\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 9s 221us/step - loss: 0.5806 - val_loss: 1.1671\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 9s 218us/step - loss: 0.5323 - val_loss: 1.1558\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 9s 220us/step - loss: 0.4890 - val_loss: 1.1262\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 9s 221us/step - loss: 0.4517 - val_loss: 1.1233\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 9s 216us/step - loss: 0.4171 - val_loss: 1.1128\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 9s 217us/step - loss: 0.3878 - val_loss: 1.1055\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 9s 218us/step - loss: 0.3608 - val_loss: 1.1014\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 9s 219us/step - loss: 0.3354 - val_loss: 1.1039\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 9s 219us/step - loss: 0.3114 - val_loss: 1.1003\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 9s 220us/step - loss: 0.2920 - val_loss: 1.1011\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 9s 220us/step - loss: 0.2722 - val_loss: 1.1008\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 9s 218us/step - loss: 0.2547 - val_loss: 1.1137\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 9s 218us/step - loss: 0.2407 - val_loss: 1.1089\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 9s 218us/step - loss: 0.2275 - val_loss: 1.1111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tR7csfYuPx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "eb87b578-1604-4a83-eb6f-7a14ff060563"
      },
      "source": [
        "trainX[10], trainX_[10], trainY[10]\n",
        "# eng_tokenizer.sequences_to_texts(trainX[10]), eng_tokenizer.sequences_to_texts(trainX_[10]), eng_tokenizer.sequences_to_texts(trainY[10])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([22, 75,  4, 76,  0,  0,  0,  0], dtype=int32),\n",
              " array([ 1, 26, 67, 37, 11,  0,  0,  0], dtype=int32),\n",
              " array([26, 67, 37, 11,  0,  0,  0,  0], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTpyAZQWGZ3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "c00a8300-f447-4b44-df06-106cd887fe77"
      },
      "source": [
        "def translate_sentence(input_seq):\n",
        "  states_value = encoder_model.predict(input_seq, use_multiprocessing=True)\n",
        "  target_seq = np.array([0] * eng_length)\n",
        "  target_seq[0] = eng_tokenizer.word_index['<sos>']\n",
        "  eos = eng_tokenizer.word_index['<eos>']\n",
        "  output_sentence = []\n",
        "\n",
        "  for _ in range(eng_length):\n",
        "      output_tokens, h, c = decoder_model.predict([target_seq.reshape(1, -1)] + states_value, use_multiprocessing=True)\n",
        "      idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "      if eos == idx:\n",
        "          break\n",
        "\n",
        "      if idx > 0:\n",
        "          word = eng_tokenizer.index_word[idx]\n",
        "          output_sentence.append(word)\n",
        "\n",
        "      target_seq[0] = idx\n",
        "      states_value = [h, c]\n",
        "\n",
        "  return ' '.join(output_sentence)\n",
        "\n",
        "a = np.random.choice(len(testX), 15)\n",
        "x = testX[a]\n",
        "\n",
        "if TF:\n",
        "  translation = []\n",
        "  for i in a:\n",
        "    input_seq = testX[i]\n",
        "    translation.append( translate_sentence(input_seq) )\n",
        "else:\n",
        "  pred_ohe = model.predict(x, use_multiprocessing=True)\n",
        "  pred = np.argmax(pred_ohe, axis=2)\n",
        "  translation = eng_tokenizer.sequences_to_texts(pred)\n",
        "\n",
        "pred_df = pd.DataFrame({'actual' : eng_tokenizer.sequences_to_texts(testY[a]), 'predicted' : translation})\n",
        "pred_df"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>where are you tom</td>\n",
              "      <td>tom is are you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>wheres it from</td>\n",
              "      <td>wheres here from from</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>he has a point</td>\n",
              "      <td>he is all wrong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>toms asleep</td>\n",
              "      <td>tom is sleeping</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i should be fine</td>\n",
              "      <td>i should be fine</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>just leave tom alone</td>\n",
              "      <td>just leave tom alone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>we cant go back</td>\n",
              "      <td>we cant go back</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>i have homework</td>\n",
              "      <td>i have homework</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>i had to stop tom</td>\n",
              "      <td>i had to stop tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>its happened</td>\n",
              "      <td>it happened</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>shes pregnant</td>\n",
              "      <td>she is</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>jam comes in a jar</td>\n",
              "      <td>is there in glass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>i mean it</td>\n",
              "      <td>i like it it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>theres no proof</td>\n",
              "      <td>theres no evidence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>im just being polite</td>\n",
              "      <td>im just polite</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  actual              predicted\n",
              "0      where are you tom         tom is are you\n",
              "1         wheres it from  wheres here from from\n",
              "2         he has a point        he is all wrong\n",
              "3            toms asleep        tom is sleeping\n",
              "4       i should be fine       i should be fine\n",
              "5   just leave tom alone   just leave tom alone\n",
              "6        we cant go back        we cant go back\n",
              "7        i have homework        i have homework\n",
              "8      i had to stop tom      i had to stop tom\n",
              "9           its happened            it happened\n",
              "10         shes pregnant                 she is\n",
              "11    jam comes in a jar      is there in glass\n",
              "12             i mean it           i like it it\n",
              "13       theres no proof     theres no evidence\n",
              "14  im just being polite         im just polite"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjP8HJluWDus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}