{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workinprogress.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygoosh/samsung/blob/master/workinprogress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_Mg8x0gASeS",
        "colab_type": "code",
        "outputId": "b4b278ec-2da4-4ece-f09c-923c25602f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from google.colab import files, drive\n",
        "from collections import defaultdict\n",
        "\n",
        "import re\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array, argmax, random, take\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, GRU, Embedding, RepeatVector, TimeDistributed\n",
        "from keras.layers import Bidirectional as Bi\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "RS = 77\n",
        "rn.seed(RS)\n",
        "# tf.random.set_seed(RS)\n",
        "np.random.seed(RS)\n",
        "np.random.RandomState(RS)\n",
        "\n",
        "gpath = Path('/content/gdrive')\n",
        "drive.mount(str(gpath))\n",
        "data_file = gpath / 'My Drive/Samsung' / 'transcriptions'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a0fc22d70461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mgpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'My Drive/Samsung'\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'transcriptions'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KByWPn1DbpS",
        "colab_type": "text"
      },
      "source": [
        "#### Let's look at the data in given file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNLzpBAGDgAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with data_file.open() as f:  \n",
        "    print(list(f.readline()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmvmq8NkDzzF",
        "colab_type": "text"
      },
      "source": [
        "#### Notice that:\n",
        "1. russian sentence is separated from transcript with '\\t'\n",
        "2. the begining and the end of transcript part are marked by '%%'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9inDldeBBz8",
        "colab_type": "text"
      },
      "source": [
        "### Let's read the data and split it into rus and trans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze9lEBcZB4A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn = []\n",
        "with data_file.open() as f:  \n",
        "  for line in f: \n",
        "    rus, trn = line.split('\\t')\n",
        "    rus_trn.append([rus.strip(), trn.strip()])\n",
        "\n",
        "print(f'Number of sentences in corpus: {len(rus_trn)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjFV-XOHBN8o",
        "colab_type": "text"
      },
      "source": [
        "##### Let's look at some sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaCdAWXfBS-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = np.random.choice(len(rus_trn), 3)\n",
        "\n",
        "print(rus_trn[a][0])\n",
        "print(rus_trn[a][1])\n",
        "print(rus_trn[b][0])\n",
        "print(rus_trn[b][1])\n",
        "print(rus_trn[c][0])\n",
        "print(rus_trn[c][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrtVAlRiBYLc",
        "colab_type": "text"
      },
      "source": [
        "#### Notice that words in russian sentence are separated by space while words in transcript are separated:\n",
        "1. by '#' in general case\n",
        "2. by '_' in case of preposition\n",
        "3. by '%% %%' in case of punctuation signs (dash, coma, etc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYfq4rSFVs6",
        "colab_type": "text"
      },
      "source": [
        "#### Let's see if we have dupliates in corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoPm3mehFfiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = array(rus_trn)\n",
        "seen_rus = Counter(a[:,0])\n",
        "seen_trn = Counter(a[:,1])\n",
        "\n",
        "print(f'Unique rus sentences: {len(seen_rus)} out of {len(rus_trn)}')\n",
        "print(f'Unique trans sentences: {len(seen_trn)} out of {len(rus_trn)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c_7KSzLGs7v",
        "colab_type": "text"
      },
      "source": [
        "#### We have a lot of duplicates! Only 3131 unique sentenses out of 50K in corpus. Please also note that some russian sentences are transcribed into different transcriptions (will look into that later on)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjR7uun4HeJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lens = [each[1] for each in seen_rus.items()]\n",
        "unq = np.unique(lens)\n",
        "qty = [lens.count(each) for each in unq]\n",
        "pd.DataFrame(qty, index=unq).plot.bar(title = 'Rus sentence repeat times', legend=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2bIKlVbIAxi",
        "colab_type": "text"
      },
      "source": [
        "#### Most duplicated sentences repeat 12 times, max up to 203 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCi44KJRImmG",
        "colab_type": "text"
      },
      "source": [
        "### Let's read that data while splitting the tokens. We'll count the tokens in each sentence. If the count in rus and trans is different, we'll record it as anomaly\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fznr-0SJ9nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_l, trn_l, anomaly = [], [], []\n",
        "seen = defaultdict(list)\n",
        "chars = Counter()\n",
        "\n",
        "for j,i in enumerate(rus_trn):\n",
        "  rus = i[0].split()\n",
        "  trn = re.split('#|_|%% %%',i[1])\n",
        "\n",
        "  if i[0] not in seen:\n",
        "    if abs(len(rus) - len(trn)) != 0: \n",
        "      anomaly.append(j)\n",
        "      # print(j, ' ', i[0])\n",
        "      # print( i[1])\n",
        "    \n",
        "    else:\n",
        "      rus_l.append(len(rus))\n",
        "      trn_l.append(len(trn))\n",
        "\n",
        "    chars += Counter(i[0])\n",
        "\n",
        "  seen[i[0]].append(j)\n",
        "\n",
        "print(f'Anomalies: {len(anomaly)}')\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "pd.DataFrame({'Number of words in Rus sentence':rus_l}).hist(ax=ax1, bins = 30);\n",
        "pd.DataFrame({'Number of words in Trns sentence':trn_l}).hist(ax=ax2, bins = 30);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxTcio_yKSbh",
        "colab_type": "text"
      },
      "source": [
        "#### So we have 49 anomalies out of 3131 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_cJH70KuSg",
        "colab_type": "text"
      },
      "source": [
        "### Let's see if we need to clean the data. First let's take a look at rus corpus alphabet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyV9anvZn187",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = sorted(chars.items())\n",
        "pd.DataFrame(s, index=(e[0]+' ' for e in s)).plot.bar(figsize=(18,4), rot=0, title = 'Char frequencies', legend=False)\n",
        "print(f'Number of times \"-\" used: {chars[\"-\"]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TZ18z9K8Qy",
        "colab_type": "text"
      },
      "source": [
        "#### Looks good! We neither have punctuations nor capital letters. The only case to check is '-' letter which is used 159 times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqkK_LRKLb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seen = set()\n",
        "seen_dash = set()\n",
        "dash_words = []\n",
        "for rus,trn in rus_trn:\n",
        "  if rus not in seen:\n",
        "    if '-' in rus:\n",
        "      ru_words = rus.split()\n",
        "      for each in ru_words:\n",
        "        if '-' in each:\n",
        "          if each not in seen_dash:\n",
        "            seen_dash.add(each)\n",
        "            dash_words.append(each)\n",
        "  else:\n",
        "    seen.add(rus)\n",
        "  \n",
        "print(dash_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VHh6JUCPGlb",
        "colab_type": "text"
      },
      "source": [
        "#### Okey, words with dash look fine, we'll consider dash as a normal letter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xAjHqyjh3x",
        "colab_type": "text"
      },
      "source": [
        "### So in order to implement autocoder for transcript we'd need to keep special symbools in transcript such as \"\\_\" and \"%% %%\". So let's create a dictionary of sentences rus <-> trans. So let's recreate the dictionary so \"\\_\" and \"%% %%\" are marked with '#' and remove begin and end markers. Plus to that let's get rid of duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7827cG05JrMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install transliterate\n",
        "# from transliterate import translit\n",
        "# translit('длавды дылпадыал лыдап', 'ru', reversed=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBgF1l93y39m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seen_rus = set()\n",
        "seen_trn = set()\n",
        "\n",
        "res, trn_l, rus_l = [], [], []\n",
        "for rus, trn in rus_trn:\n",
        "\n",
        "  if rus not in seen_rus or trn not in seen_trn:\n",
        "      seen_rus.add(rus)\n",
        "      seen_trn.add(trn)\n",
        "\n",
        "      # trn = trn.replace('%% %%', '%% %% #').replace('_', '_ #')[3:-3]\n",
        "      # rus_l.append( len(rus))\n",
        "      # trn_l.append( len(trn.split(' ')))\n",
        "      # res.append([rus, trn.split(' ')])\n",
        "\n",
        "      trn = trn.replace('%% %%', '%% %% #').replace('_', '_ #')[2:-2]\n",
        "      rus_l.append( len(rus.split()))\n",
        "      trn_l.append( len(trn.split('#')))\n",
        "      # res.append([rus.split(), ['<sos>'] + trn.split('#'), ['<sos>'] + trn.split('#')])\n",
        "      res.append([rus.split(), trn.split('#')])\n",
        "\n",
        "rus_trn_new = array(res)\n",
        "trn_length = max(trn_l)\n",
        "rus_length = max(rus_l)\n",
        "print(f'max len rus: {rus_length}, max len trn: {trn_length}')\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "pd.DataFrame({'Number of words in Rus sentence':rus_l}).hist(ax=ax1, bins = 30);\n",
        "pd.DataFrame({'Number of words in Trns sentence':trn_l}).hist(ax=ax2, bins = 30);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID8a8MoaKtVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn_new[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQ82gWtkTDx",
        "colab_type": "text"
      },
      "source": [
        "### Text to Sequence Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt-GGIUNlkCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenization(lines, split=' ', char_level=False):\n",
        "  tokenizer = Tokenizer(filters='', lower=False, split=split, char_level=char_level)\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  seq = pad_sequences(sequences=seq, maxlen=length, padding='post')\n",
        "  return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "670W8CC1llqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_tokenizer = tokenization(rus_trn_new[:, 0], split='',  char_level=True)\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
        "trn_tokenizer = tokenization(rus_trn_new[:, 1], split=' ', char_level=False)\n",
        "trn_vocab_size = len(trn_tokenizer.word_index) + 1\n",
        "print(f'Rus Vocabulary Size: {rus_vocab_size}')\n",
        "print(f'Trns Vocabulary Size: {trn_vocab_size}')\n",
        "a = np.random.choice(rus_vocab_size)\n",
        "print(trn_tokenizer.index_word[a])\n",
        "print(rus_tokenizer.index_word[a])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJdhdjvqRA9",
        "colab_type": "text"
      },
      "source": [
        "### Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaO755OI17PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn_new.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XcT-AUGqa5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(rus_trn_new, test_size=0.01, random_state = RS)\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(rus_tokenizer, rus_length, train[:, 0])\n",
        "# trainX_ = encode_sequences(trn_tokenizer, trn_length, train[:, 2])\n",
        "trainY = encode_sequences(trn_tokenizer, trn_length, train[:, 1])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(rus_tokenizer, rus_length, test[:, 0])\n",
        "# testX_ = encode_sequences(trn_tokenizer, trn_length, test[:, 2])\n",
        "testY = encode_sequences(trn_tokenizer, trn_length, test[:, 1])\n",
        "\n",
        "trainX.shape, testX.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGzWdS9cips_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(f'{t} ----> {lang.index_word[t]}')\n",
        "\n",
        "a = np.random.choice(len(trainX))\n",
        "convert(rus_tokenizer, trainX[a])\n",
        "print ()\n",
        "convert(trn_tokenizer, trainY[a])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5FSiYilVXvv",
        "colab_type": "text"
      },
      "source": [
        "#### FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8eeBkKPCPse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "rus_model = FastText(size=20)\n",
        "trn_model = FastText(size=20)\n",
        "\n",
        "rus_model.build_vocab(sentences=rus_trn_new[:, 0])\n",
        "trn_model.build_vocab(sentences=rus_trn_new[:, 1])\n",
        "\n",
        "total_rus = rus_model.corpus_total_words\n",
        "total_trn = trn_model.corpus_total_words\n",
        "print(total_rus, total_trn)\n",
        "\n",
        "rus_model.train(sentences=rus_trn_new[:,0], total_examples = rus_model.corpus_count, epochs=rus_model.iter)\n",
        "trn_model.train(sentences=rus_trn_new[:,1], total_examples = trn_model.corpus_count, epochs=trn_model.iter)\n",
        "\n",
        "print(rus_model.wv['и'])\n",
        "print(trn_model.wv[\" i _ \"])\n",
        "\n",
        "rus_model.wv.similar_by_vector(trn_model.wv[\" i _\"])\n",
        "# trn_model.wv.similar_by_vector(rus_model.wv['группа'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EovHsdWV0JJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_model.most_similar(' d' 'ix p u t a1 t ')\n",
        "# rus_model.most_similar('депутат')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8zTiADVilf",
        "colab_type": "text"
      },
      "source": [
        "### Moving on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a9-aKMvp3OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY[17]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwOy43R15_yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BS = 128\n",
        "EPOCHES = 500\n",
        "\n",
        "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  input = x = Input(shape=(None,))\n",
        "  x = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)(x)\n",
        "  x, h, c = LSTM(units, return_state=True)(x)\n",
        "  x = RepeatVector(out_timesteps)(x)\n",
        "  x = LSTM(units, return_sequences=True)(x, initial_state = (h, c))\n",
        "  output = TimeDistributed(Dense(out_vocab, activation='softmax'))(x)\n",
        "  inference_model = model = Model(input, output)\n",
        "  return model, inference_model\n",
        "\n",
        "def define_model1(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "   # encoder\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  encoder_embedding = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)\n",
        "  encoder = encoder_embedding(encoder_inputs)\n",
        "  _, state_h, state_c = LSTM(units, return_state=True)(encoder)\n",
        "  encoder_states = [state_h, state_c]\n",
        "  # decoder\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  decoder_embedding = Embedding(out_vocab, units, input_length=out_timesteps, mask_zero=True)\n",
        "  decoder = decoder_embedding(decoder_inputs)\n",
        "  decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
        "  decoder_dense = Dense(out_vocab, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  #inference model\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "  decoder_state_input_h = Input(shape=(units,))\n",
        "  decoder_state_input_c = Input(shape=(units,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_inputs_single = Input(shape=(None,))\n",
        "  decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "  decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
        "  decoder_states = [h, c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  inference_model = Model(\n",
        "      [decoder_inputs_single] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states\n",
        "  )\n",
        "  return model, inference_model\n",
        "\n",
        "model, model_inf = define_model(rus_vocab_size, trn_vocab_size, rus_length, trn_length, 128)\n",
        "model.summary()\n",
        "\n",
        "# optimizer = optimizers.RMSprop(lr=0.001)\n",
        "optimizer=optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "monitor = 'val_loss'\n",
        "mode = 'min'\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor=monitor, verbose=1, save_best_only=True, mode=mode)\n",
        "early_stop = EarlyStopping( patience=5, monitor=monitor, mode=mode)\n",
        "\n",
        "# train model\n",
        "history = model.fit(trainX, trainY.reshape(*trainY.shape, 1),\n",
        "# history = model.fit([trainX,trainY], trainY.reshape(*trainY.shape, 1),\n",
        "                    validation_data=(testX, testY.reshape(*testY.shape, 1)),\n",
        "                    # validation_data=([testX,trainY], testY.reshape(*testY.shape, 1)),\n",
        "                    epochs=EPOCHES, batch_size=BS, callbacks=[], \n",
        "                    verbose=1)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W4L0r-6Cwwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = load_model('model.h5')\n",
        "preds = model_inf.predict(testX)\n",
        "preds1 = np.argmax(preds,axis=2)\n",
        "a = np.random.choice(testY.shape[0])\n",
        "preds[a], testY[a]\n",
        "convert(trn_tokenizer, preds1[a])\n",
        "print()\n",
        "convert(rus_tokenizer, testX[a])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nTzqkY5ZtGdS",
        "colab": {}
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == n:\n",
        "          return word\n",
        "  return None\n",
        "  \n",
        "a = np.random.choice(trn_vocab_size)\n",
        "t = []\n",
        "for e in testX[0]:\n",
        "  s = get_word(e, rus_tokenizer)\n",
        "  if s: t.append(s)\n",
        "print( ' '.join(t) )\n",
        "\n",
        "t = []\n",
        "for e in testY[0]:\n",
        "  s = get_word(e, trn_tokenizer)\n",
        "  if s: t.append(s)\n",
        "print( '#'.join(t) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsT8avD8C-wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_text = []\n",
        "prd = preds[:3]\n",
        "for i in prd:\n",
        "  temp = []\n",
        "  for j in range(len(i)):\n",
        "    t = get_word(i[j], trn_tokenizer)\n",
        "    if j > 0:\n",
        "      if (t == get_word(i[j-1], trn_tokenizer)) or (t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t+'#')\n",
        "    else:\n",
        "      if(t == None):\n",
        "            temp.append('')\n",
        "      else:\n",
        "              temp.append(t+'#') \n",
        "\n",
        "  preds_text.append(''.join(temp))\n",
        "\n",
        "def unpack(tensor):\n",
        "  t = []\n",
        "  for e in tensor:\n",
        "    s = get_word(e, rus_tokenizer)\n",
        "    if s: t.append(s)\n",
        "  return ' '.join(t)\n",
        "\n",
        "preds_text\n",
        "# print( list(zip((unpack(e) for e in testX[:3]), preds_text)))\n",
        "# pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "# pred_df.sample(15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVg8UxovdB8",
        "colab_type": "text"
      },
      "source": [
        "# German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgbiEv2w_W_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IPzaHenLOFc",
        "colab_type": "code",
        "outputId": "74368bd3-ecfe-47dd-beae-23220fce74cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "  \n",
        "# split text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents\n",
        "\n",
        "# download data from http://www.manythings.org/anki/deu-eng.zip\n",
        "data = read_text(\"deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)\n",
        "\n",
        "# use first 50,000 English-German sentence pairs\n",
        "deu_eng = deu_eng[:50000,:]\n",
        "\n",
        "# Text Pre-processing\n",
        "# Remove punctuation\n",
        "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
        "deu_eng[:,2] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        " \n",
        "# convert to lowercase\n",
        "for i in range(len(deu_eng)):\n",
        "    deu_eng[i,1] = deu_eng[i,1].lower() # input (DE)\n",
        "    deu_eng[i,0] = deu_eng[i,0].lower() # target output (EN)\n",
        "\n",
        "    deu_eng[i,2] = '<sos> ' + deu_eng[i,0] # target input (EN)\n",
        "    deu_eng[i,0] = deu_eng[i,0] + ' <eos>'# target output (EN)\n",
        "\n",
        "\n",
        "# Convert text to sequence \n",
        "# empty lists\n",
        "eng_l = []\n",
        "deu_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "    eng_l.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "    deu_l.append(len(i.split()))\n",
        "\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "  \n",
        "# prepare english tokenizer\n",
        "deu_eng[0, 0] = deu_eng[0, 0] + ' <sos>'\n",
        "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "\n",
        "# prepare Deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines, sos=False, eos=False):\n",
        "    # integer encode sequences\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq\n",
        "  \n",
        "# model building\n",
        "# split data \n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "trainX_ = encode_sequences(eng_tokenizer, eng_length, train[:, 2])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
        "testX_ = encode_sequences(eng_tokenizer, eng_length, test[:, 2])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 6347\n",
            "Deutch Vocabulary Size: 10501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ4mYVSezSpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "123c801b-2dbc-4a35-d1b2-7f62e69b7f9c"
      },
      "source": [
        "deu_eng[:, 2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['<sos> hi', '<sos> hi', '<sos> run', ...,\n",
              "       '<sos> he has no specific aim', '<sos> he has only four pesos',\n",
              "       '<sos> he has stopped smoking'], dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0TnfxYFST65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "15597d21-890f-4211-c8b4-323b7e9f24af"
      },
      "source": [
        "trainX[0], trainX_[0], trainY[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 53,   3, 325, 487,   0,   0,   0,   0], dtype=int32),\n",
              " array([4301,   68,    5,   22,  437,    0,    0,    0], dtype=int32),\n",
              " array([ 68,   5,  22, 437,   1,   0,   0,   0], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3a6K0uQ-m4K",
        "colab_type": "code",
        "outputId": "f499d6f5-46c1-470e-c693-c18f44b54f17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# build NMT model\n",
        "def define_model1(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  input = x = Input(shape=(None,))\n",
        "  x = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)(x)\n",
        "  x, h, c = LSTM(units, return_state=True)(x)\n",
        "  x = RepeatVector(out_timesteps)(x)\n",
        "  x = LSTM(units, return_sequences=True)(x, initial_state = (h, c))\n",
        "  output = TimeDistributed(Dense(out_vocab, activation='softmax'))(x)\n",
        "  inference_model = model = Model(input, output)\n",
        "  return model, None, None\n",
        "\n",
        "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "   # encoder\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  encoder_embedding = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)\n",
        "  encoder = encoder_embedding(encoder_inputs)\n",
        "  _, state_h, state_c = LSTM(units, return_state=True)(encoder)\n",
        "  encoder_states = [state_h, state_c]\n",
        "  # decoder\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  decoder_embedding = Embedding(out_vocab, units, input_length=out_timesteps, mask_zero=True)\n",
        "  decoder = decoder_embedding(decoder_inputs)\n",
        "  decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
        "  decoder_dense = Dense(out_vocab, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  #inference model\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "  decoder_state_input_h = Input(shape=(units,))\n",
        "  decoder_state_input_c = Input(shape=(units,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_inputs_single = Input(shape=(None,))\n",
        "  decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "  decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
        "  decoder_states = [h, c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = Model(\n",
        "      [decoder_inputs_single] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states)\n",
        "  \n",
        "  # encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "  # decoder_state_input_h = Input(shape=(units,))\n",
        "  # decoder_state_input_c = Input(shape=(units,))\n",
        "  # decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  # decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "  #     decoder, initial_state=decoder_states_inputs)\n",
        "  # decoder_states = [state_h, state_c]\n",
        "  # decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  # decoder_model = Model(\n",
        "  #     [decoder_inputs] + decoder_states_inputs,\n",
        "  #     [decoder_outputs] + decoder_states)\n",
        "  \n",
        "  return model, encoder_model, decoder_model\n",
        "\n",
        "\n",
        "model, encoder_model, decoder_model = define_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "model.summary()\n",
        "  \n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# train model\n",
        "filename = 'model.h1.24_jan_19'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "# history = model.fit(trainX, trainY.reshape(*trainY.shape, 1), \n",
        "#           validation_data=(testX, testY.reshape(*testY.shape, 1)),\n",
        "#           epochs=30, batch_size=512, \n",
        "#           callbacks=[checkpoint], verbose=1)\n",
        "\n",
        "history = model.fit([trainX, trainX_], trainY.reshape(*trainY.shape, 1), \n",
        "          epochs=30, batch_size=512, \n",
        "          validation_data=([testX,testX_], testY.reshape(*testY.shape, 1)),\n",
        "          callbacks=[checkpoint], verbose=1)\n",
        "\n",
        "# plot validation loss vs training loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()\n",
        "\n",
        "# load saved model\n",
        "model = load_model('model.h1.24_jan_19')\n",
        "\n",
        "# make predictions\n",
        "# preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))\n",
        "# preds = model.predict_classes(testX)\n",
        "\n",
        "preds_ohe = model_inf.predict(testX)\n",
        "preds = np.argmax(preds_ohe,axis=2)\n",
        "\n",
        "def get_word(n, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == n:\n",
        "            return word\n",
        "    return None\n",
        "  \n",
        "# convert predictions into text (English)\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "    temp = []\n",
        "    for j in range(len(i)):\n",
        "        t = get_word(i[j], eng_tokenizer)\n",
        "        if j > 0:\n",
        "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)\n",
        "             \n",
        "        else:\n",
        "            if(t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)            \n",
        "        \n",
        "    preds_text.append(' '.join(temp))\n",
        "    \n",
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "\n",
        "# display results\n",
        "pred_df.sample(15)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 8, 512)       5376512     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 8, 512)       3249664     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 512), (None, 2099200     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 8, 512), (No 2099200     embedding_2[0][0]                \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8, 6347)      3256011     lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 16,080,587\n",
            "Trainable params: 16,080,587\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "40000/40000 [==============================] - 21s 517us/step - loss: 4.8902 - val_loss: 4.2351\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.23512, saving model to model.h1.24_jan_19\n",
            "Epoch 2/30\n",
            "40000/40000 [==============================] - 9s 235us/step - loss: 3.8766 - val_loss: 3.6890\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.23512 to 3.68903, saving model to model.h1.24_jan_19\n",
            "Epoch 3/30\n",
            "40000/40000 [==============================] - 9s 236us/step - loss: 3.3798 - val_loss: 3.2841\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.68903 to 3.28413, saving model to model.h1.24_jan_19\n",
            "Epoch 4/30\n",
            "40000/40000 [==============================] - 9s 235us/step - loss: 2.9579 - val_loss: 3.0014\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.28413 to 3.00138, saving model to model.h1.24_jan_19\n",
            "Epoch 5/30\n",
            "40000/40000 [==============================] - 9s 235us/step - loss: 2.6077 - val_loss: 2.7289\n",
            "\n",
            "Epoch 00005: val_loss improved from 3.00138 to 2.72894, saving model to model.h1.24_jan_19\n",
            "Epoch 6/30\n",
            "40000/40000 [==============================] - 9s 236us/step - loss: 2.3051 - val_loss: 2.4948\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.72894 to 2.49480, saving model to model.h1.24_jan_19\n",
            "Epoch 7/30\n",
            "40000/40000 [==============================] - 9s 235us/step - loss: 2.0341 - val_loss: 2.3042\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.49480 to 2.30415, saving model to model.h1.24_jan_19\n",
            "Epoch 8/30\n",
            "40000/40000 [==============================] - 10s 240us/step - loss: 1.7935 - val_loss: 2.1284\n",
            "\n",
            "Epoch 00008: val_loss improved from 2.30415 to 2.12844, saving model to model.h1.24_jan_19\n",
            "Epoch 9/30\n",
            "40000/40000 [==============================] - 10s 242us/step - loss: 1.5807 - val_loss: 1.9871\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.12844 to 1.98707, saving model to model.h1.24_jan_19\n",
            "Epoch 10/30\n",
            "40000/40000 [==============================] - 10s 239us/step - loss: 1.3947 - val_loss: 1.8698\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.98707 to 1.86978, saving model to model.h1.24_jan_19\n",
            "Epoch 11/30\n",
            "40000/40000 [==============================] - 10s 241us/step - loss: 1.2276 - val_loss: 1.8079\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.86978 to 1.80786, saving model to model.h1.24_jan_19\n",
            "Epoch 12/30\n",
            "40000/40000 [==============================] - 10s 241us/step - loss: 1.0779 - val_loss: 1.7055\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.80786 to 1.70553, saving model to model.h1.24_jan_19\n",
            "Epoch 13/30\n",
            "40000/40000 [==============================] - 10s 241us/step - loss: 0.9460 - val_loss: 1.6270\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.70553 to 1.62703, saving model to model.h1.24_jan_19\n",
            "Epoch 14/30\n",
            "40000/40000 [==============================] - 10s 241us/step - loss: 0.8288 - val_loss: 1.5644\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.62703 to 1.56445, saving model to model.h1.24_jan_19\n",
            "Epoch 15/30\n",
            "40000/40000 [==============================] - 10s 241us/step - loss: 0.7265 - val_loss: 1.5403\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.56445 to 1.54026, saving model to model.h1.24_jan_19\n",
            "Epoch 16/30\n",
            "40000/40000 [==============================] - 10s 239us/step - loss: 0.6372 - val_loss: 1.4981\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.54026 to 1.49808, saving model to model.h1.24_jan_19\n",
            "Epoch 17/30\n",
            "40000/40000 [==============================] - 10s 239us/step - loss: 0.5559 - val_loss: 1.4619\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.49808 to 1.46187, saving model to model.h1.24_jan_19\n",
            "Epoch 18/30\n",
            "40000/40000 [==============================] - 10s 240us/step - loss: 0.4866 - val_loss: 1.4399\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.46187 to 1.43985, saving model to model.h1.24_jan_19\n",
            "Epoch 19/30\n",
            "40000/40000 [==============================] - 10s 239us/step - loss: 0.4254 - val_loss: 1.4198\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.43985 to 1.41981, saving model to model.h1.24_jan_19\n",
            "Epoch 20/30\n",
            "40000/40000 [==============================] - 10s 240us/step - loss: 0.3715 - val_loss: 1.3988\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.41981 to 1.39879, saving model to model.h1.24_jan_19\n",
            "Epoch 21/30\n",
            "40000/40000 [==============================] - 10s 242us/step - loss: 0.3252 - val_loss: 1.3996\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.39879\n",
            "Epoch 22/30\n",
            "40000/40000 [==============================] - 10s 240us/step - loss: 0.2850 - val_loss: 1.3872\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.39879 to 1.38715, saving model to model.h1.24_jan_19\n",
            "Epoch 23/30\n",
            "40000/40000 [==============================] - 10s 240us/step - loss: 0.2504 - val_loss: 1.3968\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.38715\n",
            "Epoch 24/30\n",
            "40000/40000 [==============================] - 10s 241us/step - loss: 0.2203 - val_loss: 1.3888\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.38715\n",
            "Epoch 25/30\n",
            "40000/40000 [==============================] - 10s 241us/step - loss: 0.1951 - val_loss: 1.4053\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.38715\n",
            "Epoch 26/30\n",
            "40000/40000 [==============================] - 10s 239us/step - loss: 0.1746 - val_loss: 1.4066\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.38715\n",
            "Epoch 27/30\n",
            "40000/40000 [==============================] - 10s 242us/step - loss: 0.1572 - val_loss: 1.4083\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.38715\n",
            "Epoch 28/30\n",
            "40000/40000 [==============================] - 10s 240us/step - loss: 0.1417 - val_loss: 1.4200\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.38715\n",
            "Epoch 29/30\n",
            "40000/40000 [==============================] - 10s 238us/step - loss: 0.1299 - val_loss: 1.4320\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.38715\n",
            "Epoch 30/30\n",
            "40000/40000 [==============================] - 10s 240us/step - loss: 0.1202 - val_loss: 1.4257\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.38715\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD4CAYAAADIH9xYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-fb1ed981e0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# preds = model.predict_classes(testX)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0mpreds_ohe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_ohe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_inf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tR7csfYuPx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "6a3754e6-be7a-4ae2-c072-cbc1de86bc2e"
      },
      "source": [
        "trainX[10], trainX_[10], trainY[10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([22, 75,  4, 76,  0,  0,  0,  0], dtype=int32),\n",
              " array([4301,   26,   67,   37,   11,    0,    0,    0], dtype=int32),\n",
              " array([26, 67, 37, 11,  1,  0,  0,  0], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNxi-TKDNyIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "bf6d3d5f-faeb-4ec3-923b-67e5d3a65ef6"
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == n:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "preds_ohe = model_inf.predict(testX)\n",
        "preds = np.argmax(preds_ohe,axis=2)\n",
        "\n",
        "# convert predictions into text (English)\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "    temp = []\n",
        "    for j in range(len(i)):\n",
        "        t = get_word(i[j], eng_tokenizer)\n",
        "        if j > 0:\n",
        "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)\n",
        "             \n",
        "        else:\n",
        "            if(t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)            \n",
        "        \n",
        "    preds_text.append(' '.join(temp))\n",
        "    \n",
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "\n",
        "# display results\n",
        "pred_df.sample(15)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-9363071c1422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpreds_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mpred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'actual'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predicted'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpreds_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# display results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         ]\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTpyAZQWGZ3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "e10e14a5-101c-4754-ca56-cccabf20accb"
      },
      "source": [
        "def translate_sentence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
        "    eos = word2idx_outputs['<eos>']\n",
        "    output_sentence = []\n",
        "\n",
        "    for _ in range(max_out_len):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "        if eos == idx:\n",
        "            break\n",
        "\n",
        "        word = ''\n",
        "\n",
        "        if idx > 0:\n",
        "            word = trn_tokenizer.index_word[idx]\n",
        "            output_sentence.append(word)\n",
        "\n",
        "        target_seq[0, 0] = idx\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return ' '.join(output_sentence)\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, eng_vocab_size))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    # target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [input_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "i = np.random.choice(len(trainX))\n",
        "input_seq = trainX[i]\n",
        "# translation = decode_sequence(input_seq)\n",
        "\n",
        "# Encode the input as state vectors.\n",
        "states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "# Generate empty target sequence of length 1.\n",
        "target_seq = np.zeros(deu_length)\n",
        "# Populate the first character of target sequence with the start character.\n",
        "target_seq[0] = 1.\n",
        "\n",
        "# Sampling loop for a batch of sequences\n",
        "# (to simplify, here we assume a batch of size 1).\n",
        "stop_condition = False\n",
        "decoded_sentence = ''\n",
        "while not stop_condition:\n",
        "    output_tokens, h, c = decoder_model.predict(\n",
        "        [input_seq] + states_value)\n",
        "\n",
        "    # Sample a token\n",
        "    sampled_token_index = np.argmax(output_tokens, axis=2)\n",
        "    sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "    decoded_sentence += sampled_char\n",
        "\n",
        "    # Exit condition: either hit max length\n",
        "    # or find stop character.\n",
        "    if (sampled_char == '\\n' or\n",
        "        len(decoded_sentence) > max_decoder_seq_length):\n",
        "        stop_condition = True\n",
        "\n",
        "    # Update the target sequence (of length 1).\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "    # Update states\n",
        "    states_value = [h, c]\n",
        "\n",
        "\n",
        "print('-')\n",
        "print('Input:', input_sentences[i])\n",
        "print('Response:', translation)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-f2cf574588d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# Sample a token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0msampled_token_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0msampled_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverse_target_char_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msampled_char\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'reverse_target_char_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVz0gpmQVwr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}