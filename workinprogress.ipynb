{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workinprogress.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygoosh/samsung/blob/master/workinprogress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_Mg8x0gASeS",
        "colab_type": "code",
        "outputId": "c4149213-ab2f-44ae-863c-22865e142649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from google.colab import files, drive\n",
        "from collections import defaultdict\n",
        "\n",
        "import re\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array, argmax, random, take\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, LSTM, GRU, Embedding, RepeatVector, TimeDistributed\n",
        "from keras.layers import Bidirectional as Bi\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "RS = 77\n",
        "rn.seed(RS)\n",
        "# tf.random.set_seed(RS)\n",
        "np.random.seed(RS)\n",
        "np.random.RandomState(RS)\n",
        "\n",
        "gpath = Path('/content/gdrive')\n",
        "drive.mount(str(gpath))\n",
        "data_file = gpath / 'My Drive/Samsung' / 'transcriptions'"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KByWPn1DbpS",
        "colab_type": "text"
      },
      "source": [
        "#### Let's look at the data in given file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNLzpBAGDgAI",
        "colab_type": "code",
        "outputId": "d8c380dc-2b35-462f-a570-9663d469660f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "with data_file.open() as f:  \n",
        "    print(list(f.readline()))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['а', 'у', 'к', 'ц', 'и', 'о', 'н', 'ы', ' ', 'н', 'а', ' ', 'д', 'е', 'ш', 'ё', 'в', 'о', 'е', ' ', 'ж', 'и', 'л', 'ь', 'ё', ' ', 'п', 'р', 'о', 'в', 'о', 'д', 'я', 'т', 'с', 'я', ' ', 'р', 'е', 'г', 'у', 'л', 'я', 'р', 'н', 'о', '\\t', '%', '%', ' ', 'a', ' ', 'u', ' ', 'k', ' ', 't', 's', ' ', 'y', ' ', 'o', '1', ' ', 'n', ' ', 'a', 'x', ' ', '#', ' ', 'n', ' ', 'a', 'x', \"'\", ' ', '_', ' ', 'd', \"'\", ' ', \"'\", 'i', ' ', 's', 'h', ' ', 'o', '1', ' ', 'v', ' ', 'a', 'x', \"'\", ' ', 'j', 'a', 'x', ' ', '#', ' ', 'z', 'h', ' ', 'y', \"'\", ' ', 'l', \"'\", ' ', 'j', \"'\", ' ', \"'\", 'o', '1', ' ', '#', ' ', 'p', ' ', 'r', ' ', 'a', ' ', 'v', ' ', 'o', \"'\", '1', ' ', 'd', \"'\", ' ', \"'\", 'a', 'x', ' ', 't', 's', ' ', 't', 's', ' ', 'a', 'x', \"'\", ' ', '#', ' ', 'r', \"'\", ' ', \"'\", 'i', 'x', ' ', 'g', ' ', 'u', \"'\", ' ', 'l', \"'\", ' ', \"'\", 'a', '1', ' ', 'r', ' ', 'n', ' ', 'a', ' ', '%', '%', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmvmq8NkDzzF",
        "colab_type": "text"
      },
      "source": [
        "#### Notice that:\n",
        "1. russian sentence is separated from transcript with '\\t'\n",
        "2. the begining and the end of transcript part are marked by '%%'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9inDldeBBz8",
        "colab_type": "text"
      },
      "source": [
        "### Let's read the data and split it into rus and trans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze9lEBcZB4A_",
        "colab_type": "code",
        "outputId": "663828c8-415e-450d-f283-66f383a520f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "rus_trn = []\n",
        "with data_file.open() as f:  \n",
        "  for line in f: \n",
        "    rus, trn = line.split('\\t')\n",
        "    rus_trn.append([rus.strip(), trn.strip()])\n",
        "\n",
        "print(f'Number of sentences in corpus: {len(rus_trn)}')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fd37e9c1748>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 501, in __call__\n",
            "    inputs=to_list(inputs))  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 2238, in call\n",
            "    initial_state=initial_state)  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 674, in call\n",
            "    input_length=timesteps)  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 3251, in rnn\n",
            "    **while_loop_kwargs)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fd37e9cd470>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\", line 501, in __call__\n",
            "    inputs=to_list(inputs))  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 2238, in call\n",
            "    initial_state=initial_state)  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 674, in call\n",
            "    input_length=timesteps)  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 3251, in rnn\n",
            "    **while_loop_kwargs)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fd37e9c1860>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 674, in call\n",
            "    input_length=timesteps)  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 3251, in rnn\n",
            "    **while_loop_kwargs)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 1149, in unstack\n",
            "    return self._implementation.unstack(value, name=name)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fd37e9cd7f0>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/layers/recurrent.py\", line 674, in call\n",
            "    input_length=timesteps)  File \"/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\", line 3251, in rnn\n",
            "    **while_loop_kwargs)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 1149, in unstack\n",
            "    return self._implementation.unstack(value, name=name)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fd37e9c12b0>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 1149, in unstack\n",
            "    return self._implementation.unstack(value, name=name)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 320, in unstack\n",
            "    indices=math_ops.range(0, num_elements), value=value, name=name)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fd37e9cd320>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 1149, in unstack\n",
            "    return self._implementation.unstack(value, name=name)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 320, in unstack\n",
            "    indices=math_ops.range(0, num_elements), value=value, name=name)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
            "    return _add_should_use_warning(fn(*args, **kwargs))\n",
            "==================================\n",
            "Number of sentences in corpus: 50277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjFV-XOHBN8o",
        "colab_type": "text"
      },
      "source": [
        "##### Let's look at some sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaCdAWXfBS-H",
        "colab_type": "code",
        "outputId": "0549e1d1-c818-4d6e-90ac-2850cbc031b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "a,b,c = np.random.choice(len(rus_trn), 3)\n",
        "\n",
        "print(rus_trn[a][0])\n",
        "print(rus_trn[a][1])\n",
        "print(rus_trn[b][0])\n",
        "print(rus_trn[b][1])\n",
        "print(rus_trn[c][0])\n",
        "print(rus_trn[c][1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "веб-операторы обязаны ознакомить пользователя с политикой по сбору персональной информации\n",
            "%% v' 'e1 p a' p' 'i r a1 t ax r ax # a' b' 'a1 z ax n ax # a z n a k o'1 m' 'ix' t' # p o'1 l' z ax v ax' t' 'ix' l' 'ax # s _ p a' l' 'i'1 t' 'ix k ax' j' # p a _ z b o1 r ux' # p' 'ix r s a n a'1 l' n ax' j' # 'i n f a r m a1 ts ax' i %%\n",
            "цена мартовского контракта на нефть в торговой системе поднялась\n",
            "%% ts y n a1 # m a1 r t ax f s k ax v ax # k a n t r a1 k t ax # n a' _ n' 'e1 f t' # f _ t a r g o1 v ax' j' # s' 'i' s' t' 'e'1 m' 'ix # p ax' d' n' 'i l a'1 s' %%\n",
            "в августе прошлого года был достроен храм христа спасителя\n",
            "%% v _ a1 v g ux' s' t' 'ix # p r o1 sh l ax v ax # g o1 d ax # b y1 l # d a s t r o'1 jax n # h r a1 m # h r' 'i s t a1 # s p a' s' 'i'1 t' 'ix' l' 'a %%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrtVAlRiBYLc",
        "colab_type": "text"
      },
      "source": [
        "#### Notice that words in russian sentence are separated by space while words in transcript are separated:\n",
        "1. by '#' in general case\n",
        "2. by '_' in case of preposition\n",
        "3. by '%% %%' in case of punctuation signs (dash, coma, etc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYfq4rSFVs6",
        "colab_type": "text"
      },
      "source": [
        "#### Let's see if we have dupliates in corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoPm3mehFfiN",
        "colab_type": "code",
        "outputId": "b7ba6fda-1978-4dec-f903-bbb1fbe219c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "a = array(rus_trn)\n",
        "seen_rus = Counter(a[:,0])\n",
        "seen_trn = Counter(a[:,1])\n",
        "\n",
        "print(f'Unique rus sentences: {len(seen_rus)} out of {len(rus_trn)}')\n",
        "print(f'Unique trans sentences: {len(seen_trn)} out of {len(rus_trn)}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique rus sentences: 3131 out of 50277\n",
            "Unique trans sentences: 3171 out of 50277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c_7KSzLGs7v",
        "colab_type": "text"
      },
      "source": [
        "#### We have a lot of duplicates! Only 3131 unique sentenses out of 50K in corpus. Please also note that some russian sentences are transcribed into different transcriptions (will look into that later on)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjR7uun4HeJF",
        "colab_type": "code",
        "outputId": "65122b60-0fe7-40b5-d613-4423e230ddba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "lens = [each[1] for each in seen_rus.items()]\n",
        "unq = np.unique(lens)\n",
        "qty = [lens.count(each) for each in unq]\n",
        "pd.DataFrame(qty, index=unq).plot.bar(title = 'Rus sentence repeat times', legend=False);"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAERCAYAAACXT3dwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2bIKlVbIAxi",
        "colab_type": "text"
      },
      "source": [
        "#### Most duplicated sentences repeat 12 times, max up to 203 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCi44KJRImmG",
        "colab_type": "text"
      },
      "source": [
        "### Let's read that data while splitting the tokens. We'll count the tokens in each sentence. If the count in rus and trans is different, we'll record it as anomaly\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fznr-0SJ9nU",
        "colab_type": "code",
        "outputId": "7e4bf1cd-1c73-4830-b571-f22f41c1caeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "rus_l, trn_l, anomaly = [], [], []\n",
        "seen = defaultdict(list)\n",
        "chars = Counter()\n",
        "\n",
        "for j,i in enumerate(rus_trn):\n",
        "  rus = i[0].split()\n",
        "  trn = re.split('#|_|%% %%',i[1])\n",
        "\n",
        "  if i[0] not in seen:\n",
        "    if abs(len(rus) - len(trn)) != 0: \n",
        "      anomaly.append(j)\n",
        "      # print(j, ' ', i[0])\n",
        "      # print( i[1])\n",
        "    \n",
        "    else:\n",
        "      rus_l.append(len(rus))\n",
        "      trn_l.append(len(trn))\n",
        "\n",
        "    chars += Counter(i[0])\n",
        "\n",
        "  seen[i[0]].append(j)\n",
        "\n",
        "print(f'Anomalies: {len(anomaly)}')\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "pd.DataFrame({'Number of words in Rus sentence':rus_l}).hist(ax=ax1, bins = 30);\n",
        "pd.DataFrame({'Number of words in Trns sentence':trn_l}).hist(ax=ax2, bins = 30);"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anomalies: 49\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAEICAYAAAC3VYnvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxTcio_yKSbh",
        "colab_type": "text"
      },
      "source": [
        "#### So we have 49 anomalies out of 3131 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_cJH70KuSg",
        "colab_type": "text"
      },
      "source": [
        "### Let's see if we need to clean the data. First let's take a look at rus corpus alphabet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyV9anvZn187",
        "colab_type": "code",
        "outputId": "ab40081d-8578-4075-b5be-996488114632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "s = sorted(chars.items())\n",
        "pd.DataFrame(s, index=(e[0]+' ' for e in s)).plot.bar(figsize=(18,4), rot=0, title = 'Char frequencies', legend=False)\n",
        "print(f'Number of times \"-\" used: {chars[\"-\"]}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of times \"-\" used: 159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABCEAAAEICAYAAABh6e6nAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJIQQAAAAwCSEEAAAAMAkhBAAAADAJP4/AfWfTsPfcq0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1296x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TZ18z9K8Qy",
        "colab_type": "text"
      },
      "source": [
        "#### Looks good! We neither have punctuations nor capital letters. The only case to check is '-' letter which is used 159 times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqkK_LRKLb-",
        "colab_type": "code",
        "outputId": "576e8328-08ee-45d9-815b-63201fd2e28a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "seen = set()\n",
        "seen_dash = set()\n",
        "dash_words = []\n",
        "for rus,trn in rus_trn:\n",
        "  if rus not in seen:\n",
        "    if '-' in rus:\n",
        "      ru_words = rus.split()\n",
        "      for each in ru_words:\n",
        "        if '-' in each:\n",
        "          if each not in seen_dash:\n",
        "            seen_dash.add(each)\n",
        "            dash_words.append(each)\n",
        "  else:\n",
        "    seen.add(rus)\n",
        "  \n",
        "print(dash_words)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['анти-обледенитель', 'иридо-диагностика', 'дом-красавец', 'из-за', 'радио-сигналом', 'ноль-три', 'ноль-два', 'мало-обеспеченные', 'шкаф-купе', 'тех-осмотр', 'пресс-службе', 'авто-сервис', 'чуть-чуть', 'аудио-', 'какие-либо', 'де-факто', 'стерео-', 'царь-колокол', 'царь-пушка', 'гео-магнитного', 'электро-катастрофы', 'фигуристы-новобранцы', 'что-то', 'какое-либо', 'веб-операторы', 'воздушно-десантной', 'военно-морских', 'ветераны-афганцы', 'одного-единственного', 'природно-климатические', 'одном-двух', 'санкт-петербурге', 'стран-участниц', 'пять-шесть', 'по-своему', 'матч-реванш', 'словаря-справочника', 'штаб-квартире', 'чем-то', 'чего-то', 'экс-губернатора', 'российско-американский', 'северо-восточных', 'красно-белую', 'какой-нибудь', 'бело-жёлтый', 'карту-схему', 'юго-западный', 'радио-охранное', 'по-прежнему', 'каким-то', 'кино-картины', 'авто-трюки', 'пресс-конференции', 'генерал-прокурором', 'нью-йорке', 'медиа-группы', 'нью-йоркской', 'какие-то', 'санкт-петербурга', 'санкт-петербург', 'памятники-символы', 'премьер-министр', 'взлётно-посадочную', 'какого-нибудь', 'январе-феврале', 'птица-говорун', 'жучки-камнееды', 'школу-комплекс', 'художников-оформителей', 'какое-то', 'купли-продажи', 'каких-либо', 'генерал-лейтенант', 'карты-схемы', 'кому-нибудь', 'детям-инвалидам', 'как-то', 'золото-валютных', 'наконец-то', 'государственно-общественной', 'бизнес-сообщества', 'причинно-следственных', 'по-настоящему', 'юго-западного', 'премьер-министру', 'нью-йорк', 'пропан-бутановые', 'пресс-служба', 'физико-химическими', 'средне-взвешенный', 'интернет-сетью', 'экспресс-опрос', 'либерально-демократической', 'вице-премьер', 'пресс-конференция', 'военно-морского', 'по-другому', 'вице-спикера', 'радио-станции', 'конференц-зале', 'золото-валютные', 'фирм-членов', 'премьер-министром', 'химико-фармацевтической', 'весенне-полевых', 'пресс-секретарь', 'сирийско-израильский', 'юго-восток', 'сьерра-леонэ', 'южно-африканской', 'медиа-мостом', 'диско-марафон', 'видео-', 'две-три', 'экс-посол', 'военно-политического', 'компакт-диски', 'мини-электростанция', 'когда-нибудь', 'кто-то', 'веке-волкодаве', 'из-под', 'капитан-лейтенанта', 'просто-напросто', 'всё-таки', 'пресс-службы', 'южно-сахалинске', 'оперативно-следственная', 'армяно-российского', 'северо-западе']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VHh6JUCPGlb",
        "colab_type": "text"
      },
      "source": [
        "#### Okey, words with dash look fine, we'll consider dash as a normal letter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xAjHqyjh3x",
        "colab_type": "text"
      },
      "source": [
        "### So in order to implement autocoder for transcript we'd need to keep special symbools in transcript such as \"\\_\" and \"%% %%\". So let's create a dictionary of sentences rus <-> trans. So let's recreate the dictionary so \"\\_\" and \"%% %%\" are marked with '#' and remove begin and end markers. Plus to that let's get rid of duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7827cG05JrMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install transliterate\n",
        "# from transliterate import translit\n",
        "# translit('длавды дылпадыал лыдап', 'ru', reversed=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBgF1l93y39m",
        "colab_type": "code",
        "outputId": "24340397-d45a-4f3e-b948-cf2d30bd6df3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "seen_rus = set()\n",
        "seen_trn = set()\n",
        "\n",
        "res, trn_l, rus_l = [], [], []\n",
        "for rus, trn in rus_trn:\n",
        "\n",
        "  if rus not in seen_rus or trn not in seen_trn:\n",
        "      seen_rus.add(rus)\n",
        "      seen_trn.add(trn)\n",
        "\n",
        "      # trn = trn.replace('%% %%', '%% %% #').replace('_', '_ #')[3:-3]\n",
        "      # rus_l.append( len(rus))\n",
        "      # trn_l.append( len(trn.split(' ')))\n",
        "      # res.append([rus, trn.split(' ')])\n",
        "\n",
        "      trn = trn.replace('%% %%', '%% %% #').replace('_', '_ #')[2:-2]\n",
        "      rus_l.append( len(rus.split()))\n",
        "      trn_l.append( len(trn.split('#')))\n",
        "      # res.append([rus.split(), ['<sos>'] + trn.split('#'), ['<sos>'] + trn.split('#')])\n",
        "      res.append([rus.split(), ['<sos>'] + trn.split('#')])\n",
        "\n",
        "rus_trn_new = array(res)\n",
        "trn_length = max(trn_l)\n",
        "rus_length = max(rus_l)\n",
        "print(f'max len rus: {rus_length}, max len trn: {trn_length}')\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "pd.DataFrame({'Number of words in Rus sentence':rus_l}).hist(ax=ax1, bins = 30);\n",
        "pd.DataFrame({'Number of words in Trns sentence':trn_l}).hist(ax=ax2, bins = 30);"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max len rus: 15, max len trn: 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAEICAYAAAC3VYnvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKJYkSVLnWRRLkiSp8yyKJUmS1Hn/Fy/uYO6nggUhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID8a8MoaKtVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1bc5a062-9388-49ef-87cc-b0f177d0929e"
      },
      "source": [
        "rus_trn_new[0]"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['аукционы на дешёвое жильё проводятся регулярно',\n",
              "       list(['a', 'u', 'k', 'ts', 'y', 'o1', 'n', 'ax', '#', 'n', \"ax'\", '_', '#', \"d'\", \"'i\", 'sh', 'o1', 'v', \"ax'\", 'jax', '#', 'zh', \"y'\", \"l'\", \"j'\", \"'o1\", '#', 'p', 'r', 'a', 'v', \"o'1\", \"d'\", \"'ax\", 'ts', 'ts', \"ax'\", '#', \"r'\", \"'ix\", 'g', \"u'\", \"l'\", \"'a1\", 'r', 'n', 'a'])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQ82gWtkTDx",
        "colab_type": "text"
      },
      "source": [
        "### Text to Sequence Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt-GGIUNlkCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenization(lines, split=' ', char_level=False):\n",
        "  tokenizer = Tokenizer(filters='', lower=False, split=split, char_level=char_level)\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  seq = pad_sequences(sequences=seq, maxlen=length, padding='post')\n",
        "  return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "670W8CC1llqC",
        "colab_type": "code",
        "outputId": "fb095b66-3401-4d4a-fad0-ee8add416539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "rus_tokenizer = tokenization(rus_trn_new[:, 0], split='',  char_level=True)\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
        "trn_tokenizer = tokenization(rus_trn_new[:, 1], split=' ', char_level=False)\n",
        "trn_vocab_size = len(trn_tokenizer.word_index) + 1\n",
        "print(f'Rus Vocabulary Size: {rus_vocab_size}')\n",
        "print(f'Trns Vocabulary Size: {trn_vocab_size}')\n",
        "a = np.random.choice(rus_vocab_size)\n",
        "print(trn_tokenizer.index_word[a])\n",
        "print(rus_tokenizer.index_word[a])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rus Vocabulary Size: 12176\n",
            "Trns Vocabulary Size: 13890\n",
            " r' 'i k l a1 m n ax m \n",
            "реестра\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJdhdjvqRA9",
        "colab_type": "text"
      },
      "source": [
        "### Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaO755OI17PU",
        "colab_type": "code",
        "outputId": "0ddc5463-cf89-46a1-82d4-34b0646d18fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "rus_trn_new.shape"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3172, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XcT-AUGqa5v",
        "colab_type": "code",
        "outputId": "a555346c-a7c4-47f1-e09a-cd1a63436e54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train, test = train_test_split(rus_trn_new, test_size=0.001, random_state = RS)\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(rus_tokenizer, rus_length, train[:, 0])\n",
        "trainX_ = encode_sequences(trn_tokenizer, trn_length, train[:, 2])\n",
        "trainY = encode_sequences(trn_tokenizer, trn_length, train[:, 1])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(rus_tokenizer, rus_length, test[:, 0])\n",
        "testX_ = encode_sequences(trn_tokenizer, trn_length, test[:, 2])\n",
        "testY = encode_sequences(trn_tokenizer, trn_length, test[:, 1])\n",
        "\n",
        "trainX.shape, testX.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3168, 15), (4, 15))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGzWdS9cips_",
        "colab_type": "code",
        "outputId": "e8087343-63c0-4e41-ddf0-64c17346368a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(f'{t} ----> {lang.index_word[t]}')\n",
        "\n",
        "a = np.random.choice(200)\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(rus_tokenizer, trainX[a])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(trn_tokenizer, trainY[a])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "23 ----> сегодня\n",
            "45 ----> мы\n",
            "11491 ----> живём\n",
            "1 ----> в\n",
            "369 ----> одной\n",
            "11492 ----> плоскости\n",
            "39 ----> а\n",
            "77 ----> должны\n",
            "620 ----> жить\n",
            "11493 ----> объёмно\n",
            "\n",
            "Target Language; index to word mapping\n",
            "40 ---->  s' 'i v o'1 d' n' 'ax \n",
            "58 ---->  m y1 \n",
            "13046 ---->  zh y' v' 'o1 m \n",
            "1 ---->  v _ \n",
            "329 ---->  a d n o'1 j' \n",
            "13047 ---->  p l o1 s k ax' s' t' 'i %% %% \n",
            "9 ---->  a _ \n",
            "83 ---->  d a l zh n y1 \n",
            "548 ---->  zh y'1 t' \n",
            "13048 ---->  'a b j' 'o1 m n a \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5FSiYilVXvv",
        "colab_type": "text"
      },
      "source": [
        "#### FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8eeBkKPCPse",
        "colab_type": "code",
        "outputId": "910e9689-034e-4bcf-ec98-56440f22f26e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "rus_model = FastText(size=20)\n",
        "trn_model = FastText(size=20)\n",
        "\n",
        "rus_model.build_vocab(sentences=rus_trn_new[:, 0])\n",
        "trn_model.build_vocab(sentences=rus_trn_new[:, 1])\n",
        "\n",
        "total_rus = rus_model.corpus_total_words\n",
        "total_trn = trn_model.corpus_total_words\n",
        "print(total_rus, total_trn)\n",
        "\n",
        "rus_model.train(sentences=rus_trn_new[:,0], total_examples = rus_model.corpus_count, epochs=rus_model.iter)\n",
        "trn_model.train(sentences=rus_trn_new[:,1], total_examples = trn_model.corpus_count, epochs=trn_model.iter)\n",
        "\n",
        "print(rus_model.wv['и'])\n",
        "print(trn_model.wv[\" i _ \"])\n",
        "\n",
        "rus_model.wv.similar_by_vector(trn_model.wv[\" i _\"])\n",
        "# trn_model.wv.similar_by_vector(rus_model.wv['группа'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32448 32395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[-0.69065756  0.4192582  -1.2095522  -0.01563328 -0.5254309   0.5680641\n",
            " -0.65832     0.16494563 -0.406429    0.3612038  -1.1729337   0.1500966\n",
            "  0.4335361  -0.03284142 -0.88770705  0.15605743  0.08337434 -0.57792157\n",
            " -1.086083    0.5548296 ]\n",
            "[ 0.56364745  0.70111406 -0.36694923  0.05703797 -0.52206254  0.79194874\n",
            " -1.2288195  -0.7276088   0.29206008  0.5571377  -0.46059236  0.7377961\n",
            " -0.00935896  1.1696087   1.1398089  -0.6379641  -0.2372518  -0.03386954\n",
            " -0.6044104   0.5300397 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('я', 0.3086957335472107),\n",
              " ('ряд', 0.30652180314064026),\n",
              " ('им', 0.2938210964202881),\n",
              " ('даёт', 0.2934850752353668),\n",
              " ('кредит', 0.2921362817287445),\n",
              " ('фонд', 0.2906072735786438),\n",
              " ('чечни', 0.28800374269485474),\n",
              " ('вызывает', 0.28710639476776123),\n",
              " ('открытие', 0.28527945280075073),\n",
              " ('роль', 0.28502026200294495)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EovHsdWV0JJg",
        "colab_type": "code",
        "outputId": "3f49f479-aa94-4e4f-8912-cccefb9f63d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "trn_model.most_similar(' d' 'ix p u t a1 t ')\n",
        "# rus_model.most_similar('депутат')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(' i _ ', 0.9997418522834778),\n",
              " (\" b' 'u d zh e1 t \", 0.9996572136878967),\n",
              " (\" z' _ \", 0.9996542930603027),\n",
              " (\" n' 'e1 t \", 0.9996196031570435),\n",
              " (' e1 t ax ', 0.9996190071105957),\n",
              " (\" 'i _ \", 0.9996137619018555),\n",
              " (\" s' _ \", 0.9996098875999451),\n",
              " (\" d' 'e'1 n' \", 0.9996077418327332),\n",
              " (\" f s' 'e1 h \", 0.999605119228363),\n",
              " (\" sh' _ \", 0.9996048212051392)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8zTiADVilf",
        "colab_type": "text"
      },
      "source": [
        "### Moving on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TKxLLWFseI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convert(trn_tokenizer, trainY[17])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwOy43R15_yn",
        "colab_type": "code",
        "outputId": "1b0a2372-b201-4abb-8729-44f6f9031b40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "BS = 128\n",
        "EPOCHES = 500\n",
        "\n",
        "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "  model.add(LSTM(units))\n",
        "  model.add(RepeatVector(out_timesteps))\n",
        "  model.add(LSTM(units, return_sequences=True))\n",
        "  model.add(TimeDistributed(Dense(out_vocab, activation='softmax')))\n",
        "  return model\n",
        "\n",
        "def define_model1(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "    # encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)\n",
        "    encoder = encoder_embedding(encoder_inputs)\n",
        "    _, state_h, state_c = LSTM(units, return_state=True)(encoder)\n",
        "    # decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(out_vocab, units, input_length=out_timesteps, mask_zero=True)\n",
        "    decoder = decoder_embedding(decoder_inputs)\n",
        "    decoder_outputs, _, _ = LSTM(units, return_sequences=True, return_state=True)(decoder, initial_state=(state_h, state_c))\n",
        "    decoder_outputs = Dense(out_vocab, activation='softmax')(decoder_outputs)\n",
        "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model = define_model(rus_vocab_size, trn_vocab_size, rus_length, trn_length, 512)\n",
        "model.summary()\n",
        "\n",
        "# optimizer = optimizers.RMSprop(lr=0.001)\n",
        "optimizer=optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "# model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "\n",
        "monitor = 'val_loss'\n",
        "mode = 'min'\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor=monitor, verbose=1, save_best_only=True, mode=mode)\n",
        "early_stop = EarlyStopping( patience=5, monitor=monitor, mode=mode)\n",
        "\n",
        "# train model\n",
        "history = model.fit(trainX, trainY.reshape(*trainY.shape, 1),\n",
        "# history = model.fit([trainX,trainX_], trainY.reshape(*trainY.shape, 1),\n",
        "                    validation_data=(testX, testY.reshape(*testY.shape, 1)),\n",
        "                    # validation_data=([testX,testX_], testY.reshape(*testY.shape, 1)),\n",
        "                    epochs=EPOCHES, batch_size=BS, callbacks=[], \n",
        "                    verbose=1)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_37 (Embedding)     (None, 15, 512)           6234112   \n",
            "_________________________________________________________________\n",
            "lstm_44 (LSTM)               (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector_8 (RepeatVecto (None, 15, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_45 (LSTM)               (None, 15, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 15, 13890)         7125570   \n",
            "=================================================================\n",
            "Total params: 17,558,082\n",
            "Trainable params: 17,558,082\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 3168 samples, validate on 4 samples\n",
            "Epoch 1/500\n",
            "3168/3168 [==============================] - 15s 5ms/step - loss: 7.1480 - val_loss: 5.9683\n",
            "Epoch 2/500\n",
            "3168/3168 [==============================] - 3s 916us/step - loss: 5.9195 - val_loss: 5.4554\n",
            "Epoch 3/500\n",
            "3168/3168 [==============================] - 3s 891us/step - loss: 5.7376 - val_loss: 5.6131\n",
            "Epoch 4/500\n",
            "3168/3168 [==============================] - 3s 888us/step - loss: 5.6424 - val_loss: 5.4845\n",
            "Epoch 5/500\n",
            "3168/3168 [==============================] - 3s 891us/step - loss: 5.5048 - val_loss: 5.5354\n",
            "Epoch 6/500\n",
            "3168/3168 [==============================] - 3s 888us/step - loss: 5.3856 - val_loss: 5.5479\n",
            "Epoch 7/500\n",
            "3168/3168 [==============================] - 3s 896us/step - loss: 5.2828 - val_loss: 5.5985\n",
            "Epoch 8/500\n",
            "3168/3168 [==============================] - 3s 906us/step - loss: 5.2169 - val_loss: 5.7063\n",
            "Epoch 9/500\n",
            "3168/3168 [==============================] - 3s 897us/step - loss: 5.1400 - val_loss: 5.8062\n",
            "Epoch 10/500\n",
            "3168/3168 [==============================] - 3s 909us/step - loss: 5.0570 - val_loss: 5.9408\n",
            "Epoch 11/500\n",
            "3168/3168 [==============================] - 3s 902us/step - loss: 4.9688 - val_loss: 6.1354\n",
            "Epoch 12/500\n",
            "3168/3168 [==============================] - 3s 895us/step - loss: 4.9054 - val_loss: 6.0929\n",
            "Epoch 13/500\n",
            "3168/3168 [==============================] - 3s 901us/step - loss: 4.8372 - val_loss: 6.0699\n",
            "Epoch 14/500\n",
            "3168/3168 [==============================] - 3s 914us/step - loss: 4.7707 - val_loss: 6.0187\n",
            "Epoch 15/500\n",
            "3168/3168 [==============================] - 3s 899us/step - loss: 4.7133 - val_loss: 6.1533\n",
            "Epoch 16/500\n",
            "3168/3168 [==============================] - 3s 873us/step - loss: 4.6634 - val_loss: 5.9795\n",
            "Epoch 17/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 4.6135 - val_loss: 5.9870\n",
            "Epoch 18/500\n",
            "3168/3168 [==============================] - 3s 874us/step - loss: 4.5991 - val_loss: 6.0664\n",
            "Epoch 19/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 4.5546 - val_loss: 6.0622\n",
            "Epoch 20/500\n",
            "3168/3168 [==============================] - 3s 877us/step - loss: 4.5279 - val_loss: 6.1179\n",
            "Epoch 21/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 4.4972 - val_loss: 5.9503\n",
            "Epoch 22/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 4.4582 - val_loss: 5.9052\n",
            "Epoch 23/500\n",
            "3168/3168 [==============================] - 3s 885us/step - loss: 4.4250 - val_loss: 5.9204\n",
            "Epoch 24/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 4.3949 - val_loss: 5.9219\n",
            "Epoch 25/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 4.3685 - val_loss: 5.8966\n",
            "Epoch 26/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 4.3393 - val_loss: 5.8916\n",
            "Epoch 27/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 4.3081 - val_loss: 5.9015\n",
            "Epoch 28/500\n",
            "3168/3168 [==============================] - 3s 844us/step - loss: 4.2817 - val_loss: 5.8536\n",
            "Epoch 29/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 4.2998 - val_loss: 5.8571\n",
            "Epoch 30/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 4.3034 - val_loss: 5.8481\n",
            "Epoch 31/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 4.2528 - val_loss: 5.8611\n",
            "Epoch 32/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 4.2123 - val_loss: 5.8492\n",
            "Epoch 33/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 4.1788 - val_loss: 5.8493\n",
            "Epoch 34/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 4.1566 - val_loss: 5.8303\n",
            "Epoch 35/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 4.1287 - val_loss: 5.8366\n",
            "Epoch 36/500\n",
            "3168/3168 [==============================] - 3s 879us/step - loss: 4.1039 - val_loss: 5.8494\n",
            "Epoch 37/500\n",
            "3168/3168 [==============================] - 3s 879us/step - loss: 4.0765 - val_loss: 5.8169\n",
            "Epoch 38/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 4.0531 - val_loss: 5.8241\n",
            "Epoch 39/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 4.0266 - val_loss: 5.8317\n",
            "Epoch 40/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 4.0055 - val_loss: 5.8722\n",
            "Epoch 41/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 3.9893 - val_loss: 5.8302\n",
            "Epoch 42/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 3.9692 - val_loss: 5.8328\n",
            "Epoch 43/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 3.9618 - val_loss: 5.8618\n",
            "Epoch 44/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 3.9344 - val_loss: 5.8435\n",
            "Epoch 45/500\n",
            "3168/3168 [==============================] - 3s 873us/step - loss: 3.9030 - val_loss: 5.8487\n",
            "Epoch 46/500\n",
            "3168/3168 [==============================] - 3s 882us/step - loss: 3.8783 - val_loss: 5.8319\n",
            "Epoch 47/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 3.8515 - val_loss: 5.8720\n",
            "Epoch 48/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 3.8292 - val_loss: 5.8585\n",
            "Epoch 49/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 3.8039 - val_loss: 5.8790\n",
            "Epoch 50/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 3.7871 - val_loss: 5.8704\n",
            "Epoch 51/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 3.7693 - val_loss: 5.8871\n",
            "Epoch 52/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 3.7822 - val_loss: 5.8847\n",
            "Epoch 53/500\n",
            "3168/3168 [==============================] - 3s 878us/step - loss: 3.7487 - val_loss: 5.9101\n",
            "Epoch 54/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 3.7124 - val_loss: 5.8885\n",
            "Epoch 55/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 3.6852 - val_loss: 5.9189\n",
            "Epoch 56/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 3.6611 - val_loss: 5.9103\n",
            "Epoch 57/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 3.6380 - val_loss: 5.9123\n",
            "Epoch 58/500\n",
            "3168/3168 [==============================] - 3s 890us/step - loss: 3.6155 - val_loss: 5.9353\n",
            "Epoch 59/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 3.5919 - val_loss: 5.9107\n",
            "Epoch 60/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 3.5716 - val_loss: 5.9199\n",
            "Epoch 61/500\n",
            "3168/3168 [==============================] - 3s 875us/step - loss: 3.5496 - val_loss: 5.9289\n",
            "Epoch 62/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 3.5247 - val_loss: 5.9333\n",
            "Epoch 63/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 3.5002 - val_loss: 5.9515\n",
            "Epoch 64/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 3.4773 - val_loss: 5.9568\n",
            "Epoch 65/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 3.4657 - val_loss: 6.0123\n",
            "Epoch 66/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 3.4580 - val_loss: 5.9772\n",
            "Epoch 67/500\n",
            "3168/3168 [==============================] - 3s 883us/step - loss: 3.4242 - val_loss: 5.9883\n",
            "Epoch 68/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 3.3884 - val_loss: 5.9660\n",
            "Epoch 69/500\n",
            "3168/3168 [==============================] - 3s 878us/step - loss: 3.3616 - val_loss: 5.9694\n",
            "Epoch 70/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 3.3407 - val_loss: 6.0178\n",
            "Epoch 71/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 3.3129 - val_loss: 5.9785\n",
            "Epoch 72/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 3.2858 - val_loss: 6.0044\n",
            "Epoch 73/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 3.2582 - val_loss: 5.9876\n",
            "Epoch 74/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 3.2321 - val_loss: 5.9803\n",
            "Epoch 75/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 3.2066 - val_loss: 5.9922\n",
            "Epoch 76/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 3.1800 - val_loss: 5.9897\n",
            "Epoch 77/500\n",
            "3168/3168 [==============================] - 3s 874us/step - loss: 3.1552 - val_loss: 6.0032\n",
            "Epoch 78/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 3.1417 - val_loss: 5.9781\n",
            "Epoch 79/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 3.1271 - val_loss: 6.0246\n",
            "Epoch 80/500\n",
            "3168/3168 [==============================] - 3s 891us/step - loss: 3.0910 - val_loss: 5.9653\n",
            "Epoch 81/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 3.0507 - val_loss: 5.9848\n",
            "Epoch 82/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 3.0167 - val_loss: 5.9699\n",
            "Epoch 83/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 2.9864 - val_loss: 6.0095\n",
            "Epoch 84/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 2.9582 - val_loss: 5.9828\n",
            "Epoch 85/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 2.9304 - val_loss: 5.9904\n",
            "Epoch 86/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 2.9041 - val_loss: 5.9928\n",
            "Epoch 87/500\n",
            "3168/3168 [==============================] - 3s 879us/step - loss: 2.8784 - val_loss: 6.0025\n",
            "Epoch 88/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 2.8571 - val_loss: 6.0017\n",
            "Epoch 89/500\n",
            "3168/3168 [==============================] - 3s 876us/step - loss: 2.8501 - val_loss: 6.0294\n",
            "Epoch 90/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 2.8370 - val_loss: 6.0178\n",
            "Epoch 91/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 2.8029 - val_loss: 6.0200\n",
            "Epoch 92/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 2.7767 - val_loss: 6.0291\n",
            "Epoch 93/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 2.7477 - val_loss: 6.0391\n",
            "Epoch 94/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 2.7166 - val_loss: 6.0378\n",
            "Epoch 95/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 2.6870 - val_loss: 6.0451\n",
            "Epoch 96/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 2.6558 - val_loss: 6.0601\n",
            "Epoch 97/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 2.6261 - val_loss: 6.0366\n",
            "Epoch 98/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 2.5960 - val_loss: 6.0480\n",
            "Epoch 99/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 2.5734 - val_loss: 6.0598\n",
            "Epoch 100/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 2.5555 - val_loss: 6.0693\n",
            "Epoch 101/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 2.5347 - val_loss: 6.0580\n",
            "Epoch 102/500\n",
            "3168/3168 [==============================] - 3s 876us/step - loss: 2.5120 - val_loss: 6.0086\n",
            "Epoch 103/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 2.4894 - val_loss: 6.0474\n",
            "Epoch 104/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 2.4757 - val_loss: 6.0038\n",
            "Epoch 105/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 2.4470 - val_loss: 6.0632\n",
            "Epoch 106/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 2.4205 - val_loss: 6.0712\n",
            "Epoch 107/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 2.3957 - val_loss: 6.0370\n",
            "Epoch 108/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 2.3761 - val_loss: 6.0689\n",
            "Epoch 109/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 2.3839 - val_loss: 6.1013\n",
            "Epoch 110/500\n",
            "3168/3168 [==============================] - 3s 873us/step - loss: 2.3798 - val_loss: 6.1236\n",
            "Epoch 111/500\n",
            "3168/3168 [==============================] - 3s 883us/step - loss: 2.3871 - val_loss: 6.0461\n",
            "Epoch 112/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 2.3334 - val_loss: 6.1137\n",
            "Epoch 113/500\n",
            "3168/3168 [==============================] - 3s 881us/step - loss: 2.2852 - val_loss: 6.1020\n",
            "Epoch 114/500\n",
            "3168/3168 [==============================] - 3s 882us/step - loss: 2.2532 - val_loss: 6.0739\n",
            "Epoch 115/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 2.2259 - val_loss: 6.0714\n",
            "Epoch 116/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 2.1975 - val_loss: 6.0858\n",
            "Epoch 117/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 2.1767 - val_loss: 6.0885\n",
            "Epoch 118/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 2.1504 - val_loss: 6.0951\n",
            "Epoch 119/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 2.1304 - val_loss: 6.0938\n",
            "Epoch 120/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 2.1064 - val_loss: 6.0898\n",
            "Epoch 121/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 2.0803 - val_loss: 6.0987\n",
            "Epoch 122/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 2.0562 - val_loss: 6.1000\n",
            "Epoch 123/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 2.0366 - val_loss: 6.0955\n",
            "Epoch 124/500\n",
            "3168/3168 [==============================] - 3s 877us/step - loss: 2.0119 - val_loss: 6.0917\n",
            "Epoch 125/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 1.9974 - val_loss: 6.1194\n",
            "Epoch 126/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 1.9878 - val_loss: 6.0802\n",
            "Epoch 127/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 1.9784 - val_loss: 6.1039\n",
            "Epoch 128/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 1.9624 - val_loss: 6.1064\n",
            "Epoch 129/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 1.9410 - val_loss: 6.1131\n",
            "Epoch 130/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 1.9074 - val_loss: 6.0987\n",
            "Epoch 131/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 1.8739 - val_loss: 6.1031\n",
            "Epoch 132/500\n",
            "3168/3168 [==============================] - 3s 872us/step - loss: 1.8452 - val_loss: 6.1695\n",
            "Epoch 133/500\n",
            "3168/3168 [==============================] - 3s 877us/step - loss: 1.8210 - val_loss: 6.1035\n",
            "Epoch 134/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 1.7931 - val_loss: 6.1191\n",
            "Epoch 135/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 1.7674 - val_loss: 6.1358\n",
            "Epoch 136/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 1.7481 - val_loss: 6.1557\n",
            "Epoch 137/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 1.7260 - val_loss: 6.1397\n",
            "Epoch 138/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 1.7032 - val_loss: 6.1337\n",
            "Epoch 139/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 1.6808 - val_loss: 6.1552\n",
            "Epoch 140/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 1.6569 - val_loss: 6.1894\n",
            "Epoch 141/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 1.6314 - val_loss: 6.1613\n",
            "Epoch 142/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 1.6102 - val_loss: 6.1270\n",
            "Epoch 143/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 1.5897 - val_loss: 6.1548\n",
            "Epoch 144/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 1.5688 - val_loss: 6.1450\n",
            "Epoch 145/500\n",
            "3168/3168 [==============================] - 3s 875us/step - loss: 1.5426 - val_loss: 6.1825\n",
            "Epoch 146/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 1.5206 - val_loss: 6.1570\n",
            "Epoch 147/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 1.4961 - val_loss: 6.1818\n",
            "Epoch 148/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 1.4808 - val_loss: 6.1566\n",
            "Epoch 149/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 1.4631 - val_loss: 6.1316\n",
            "Epoch 150/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 1.4370 - val_loss: 6.2343\n",
            "Epoch 151/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 1.4135 - val_loss: 6.1929\n",
            "Epoch 152/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 1.3912 - val_loss: 6.1862\n",
            "Epoch 153/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 1.3664 - val_loss: 6.2422\n",
            "Epoch 154/500\n",
            "3168/3168 [==============================] - 3s 883us/step - loss: 1.3387 - val_loss: 6.2580\n",
            "Epoch 155/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 1.3189 - val_loss: 6.2379\n",
            "Epoch 156/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 1.2948 - val_loss: 6.2660\n",
            "Epoch 157/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 1.2680 - val_loss: 6.2836\n",
            "Epoch 158/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 1.2491 - val_loss: 6.2652\n",
            "Epoch 159/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 1.2310 - val_loss: 6.2755\n",
            "Epoch 160/500\n",
            "3168/3168 [==============================] - 3s 874us/step - loss: 1.2161 - val_loss: 6.2695\n",
            "Epoch 161/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 1.1893 - val_loss: 6.2728\n",
            "Epoch 162/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 1.1624 - val_loss: 6.2760\n",
            "Epoch 163/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 1.1402 - val_loss: 6.2803\n",
            "Epoch 164/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 1.1175 - val_loss: 6.2842\n",
            "Epoch 165/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 1.0933 - val_loss: 6.2763\n",
            "Epoch 166/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 1.0701 - val_loss: 6.2810\n",
            "Epoch 167/500\n",
            "3168/3168 [==============================] - 3s 875us/step - loss: 1.0522 - val_loss: 6.2775\n",
            "Epoch 168/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 1.0314 - val_loss: 6.2856\n",
            "Epoch 169/500\n",
            "3168/3168 [==============================] - 3s 880us/step - loss: 1.0128 - val_loss: 6.2904\n",
            "Epoch 170/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 1.0037 - val_loss: 6.2983\n",
            "Epoch 171/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.9917 - val_loss: 6.3245\n",
            "Epoch 172/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 0.9724 - val_loss: 6.2887\n",
            "Epoch 173/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.9522 - val_loss: 6.2857\n",
            "Epoch 174/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.9273 - val_loss: 6.2869\n",
            "Epoch 175/500\n",
            "3168/3168 [==============================] - 3s 881us/step - loss: 0.9060 - val_loss: 6.3201\n",
            "Epoch 176/500\n",
            "3168/3168 [==============================] - 3s 874us/step - loss: 0.8879 - val_loss: 6.3123\n",
            "Epoch 177/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 0.8660 - val_loss: 6.3119\n",
            "Epoch 178/500\n",
            "3168/3168 [==============================] - 3s 872us/step - loss: 0.8461 - val_loss: 6.2902\n",
            "Epoch 179/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.8270 - val_loss: 6.3216\n",
            "Epoch 180/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.8085 - val_loss: 6.3153\n",
            "Epoch 181/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.7942 - val_loss: 6.3160\n",
            "Epoch 182/500\n",
            "3168/3168 [==============================] - 3s 873us/step - loss: 0.7792 - val_loss: 6.3312\n",
            "Epoch 183/500\n",
            "3168/3168 [==============================] - 3s 878us/step - loss: 0.7635 - val_loss: 6.3858\n",
            "Epoch 184/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 0.7531 - val_loss: 6.3368\n",
            "Epoch 185/500\n",
            "3168/3168 [==============================] - 3s 873us/step - loss: 0.7494 - val_loss: 6.3522\n",
            "Epoch 186/500\n",
            "3168/3168 [==============================] - 3s 881us/step - loss: 0.7419 - val_loss: 6.3302\n",
            "Epoch 187/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.7273 - val_loss: 6.3046\n",
            "Epoch 188/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.7105 - val_loss: 6.3071\n",
            "Epoch 189/500\n",
            "3168/3168 [==============================] - 3s 875us/step - loss: 0.6910 - val_loss: 6.3257\n",
            "Epoch 190/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 0.6739 - val_loss: 6.3544\n",
            "Epoch 191/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 0.6557 - val_loss: 6.3087\n",
            "Epoch 192/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.6430 - val_loss: 6.3225\n",
            "Epoch 193/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.6310 - val_loss: 6.3537\n",
            "Epoch 194/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 0.6191 - val_loss: 6.3578\n",
            "Epoch 195/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.6013 - val_loss: 6.3168\n",
            "Epoch 196/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 0.5866 - val_loss: 6.3681\n",
            "Epoch 197/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 0.5728 - val_loss: 6.3647\n",
            "Epoch 198/500\n",
            "3168/3168 [==============================] - 3s 881us/step - loss: 0.5607 - val_loss: 6.3719\n",
            "Epoch 199/500\n",
            "3168/3168 [==============================] - 3s 889us/step - loss: 0.5504 - val_loss: 6.3434\n",
            "Epoch 200/500\n",
            "3168/3168 [==============================] - 3s 876us/step - loss: 0.5442 - val_loss: 6.3604\n",
            "Epoch 201/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.5387 - val_loss: 6.3965\n",
            "Epoch 202/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 0.5313 - val_loss: 6.3657\n",
            "Epoch 203/500\n",
            "3168/3168 [==============================] - 3s 880us/step - loss: 0.5260 - val_loss: 6.3805\n",
            "Epoch 204/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 0.5190 - val_loss: 6.4107\n",
            "Epoch 205/500\n",
            "3168/3168 [==============================] - 3s 873us/step - loss: 0.5088 - val_loss: 6.3450\n",
            "Epoch 206/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.5045 - val_loss: 6.3757\n",
            "Epoch 207/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.4952 - val_loss: 6.3620\n",
            "Epoch 208/500\n",
            "3168/3168 [==============================] - 3s 872us/step - loss: 0.4760 - val_loss: 6.3801\n",
            "Epoch 209/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.4612 - val_loss: 6.4155\n",
            "Epoch 210/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.4506 - val_loss: 6.3765\n",
            "Epoch 211/500\n",
            "3168/3168 [==============================] - 3s 889us/step - loss: 0.4366 - val_loss: 6.3657\n",
            "Epoch 212/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.4256 - val_loss: 6.3923\n",
            "Epoch 213/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.4160 - val_loss: 6.4163\n",
            "Epoch 214/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.4070 - val_loss: 6.4373\n",
            "Epoch 215/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 0.3978 - val_loss: 6.4032\n",
            "Epoch 216/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 0.3884 - val_loss: 6.4451\n",
            "Epoch 217/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.3815 - val_loss: 6.4562\n",
            "Epoch 218/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 0.3741 - val_loss: 6.4726\n",
            "Epoch 219/500\n",
            "3168/3168 [==============================] - 3s 876us/step - loss: 0.3649 - val_loss: 6.4233\n",
            "Epoch 220/500\n",
            "3168/3168 [==============================] - 3s 888us/step - loss: 0.3571 - val_loss: 6.4736\n",
            "Epoch 221/500\n",
            "3168/3168 [==============================] - 3s 877us/step - loss: 0.3542 - val_loss: 6.4395\n",
            "Epoch 222/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 0.3524 - val_loss: 6.4581\n",
            "Epoch 223/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.3460 - val_loss: 6.4692\n",
            "Epoch 224/500\n",
            "3168/3168 [==============================] - 3s 877us/step - loss: 0.3385 - val_loss: 6.4753\n",
            "Epoch 225/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 0.3302 - val_loss: 6.4167\n",
            "Epoch 226/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.3243 - val_loss: 6.4391\n",
            "Epoch 227/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.3187 - val_loss: 6.4879\n",
            "Epoch 228/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.3103 - val_loss: 6.4892\n",
            "Epoch 229/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.3041 - val_loss: 6.4692\n",
            "Epoch 230/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 0.2989 - val_loss: 6.5054\n",
            "Epoch 231/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 0.2954 - val_loss: 6.4557\n",
            "Epoch 232/500\n",
            "3168/3168 [==============================] - 3s 876us/step - loss: 0.2919 - val_loss: 6.4703\n",
            "Epoch 233/500\n",
            "3168/3168 [==============================] - 3s 884us/step - loss: 0.2864 - val_loss: 6.4785\n",
            "Epoch 234/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.2845 - val_loss: 6.5909\n",
            "Epoch 235/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.2828 - val_loss: 6.4658\n",
            "Epoch 236/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.2864 - val_loss: 6.5283\n",
            "Epoch 237/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 0.2880 - val_loss: 6.5580\n",
            "Epoch 238/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 0.2854 - val_loss: 6.5265\n",
            "Epoch 239/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 0.2771 - val_loss: 6.5983\n",
            "Epoch 240/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.2655 - val_loss: 6.5862\n",
            "Epoch 241/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.2507 - val_loss: 6.5654\n",
            "Epoch 242/500\n",
            "3168/3168 [==============================] - 3s 879us/step - loss: 0.2409 - val_loss: 6.5970\n",
            "Epoch 243/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.2306 - val_loss: 6.5979\n",
            "Epoch 244/500\n",
            "3168/3168 [==============================] - 3s 849us/step - loss: 0.2224 - val_loss: 6.6215\n",
            "Epoch 245/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 0.2171 - val_loss: 6.5860\n",
            "Epoch 246/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.2192 - val_loss: 6.6252\n",
            "Epoch 247/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.2233 - val_loss: 6.5688\n",
            "Epoch 248/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.2244 - val_loss: 6.5276\n",
            "Epoch 249/500\n",
            "3168/3168 [==============================] - 3s 843us/step - loss: 0.2282 - val_loss: 6.5535\n",
            "Epoch 250/500\n",
            "3168/3168 [==============================] - 3s 842us/step - loss: 0.2230 - val_loss: 6.5608\n",
            "Epoch 251/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.2174 - val_loss: 6.5634\n",
            "Epoch 252/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.2140 - val_loss: 6.5849\n",
            "Epoch 253/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 0.2097 - val_loss: 6.5802\n",
            "Epoch 254/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 0.2023 - val_loss: 6.6057\n",
            "Epoch 255/500\n",
            "3168/3168 [==============================] - 3s 876us/step - loss: 0.1940 - val_loss: 6.6086\n",
            "Epoch 256/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.1956 - val_loss: 6.5162\n",
            "Epoch 257/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 0.1965 - val_loss: 6.5413\n",
            "Epoch 258/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.1909 - val_loss: 6.5628\n",
            "Epoch 259/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.1841 - val_loss: 6.5029\n",
            "Epoch 260/500\n",
            "3168/3168 [==============================] - 3s 842us/step - loss: 0.1785 - val_loss: 6.5115\n",
            "Epoch 261/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.1745 - val_loss: 6.4824\n",
            "Epoch 262/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.1726 - val_loss: 6.5351\n",
            "Epoch 263/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 0.1690 - val_loss: 6.5083\n",
            "Epoch 264/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 0.1649 - val_loss: 6.5876\n",
            "Epoch 265/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 0.1583 - val_loss: 6.5181\n",
            "Epoch 266/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.1549 - val_loss: 6.5037\n",
            "Epoch 267/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.1507 - val_loss: 6.5348\n",
            "Epoch 268/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.1465 - val_loss: 6.5103\n",
            "Epoch 269/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.1428 - val_loss: 6.5467\n",
            "Epoch 270/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.1397 - val_loss: 6.5345\n",
            "Epoch 271/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 0.1366 - val_loss: 6.5273\n",
            "Epoch 272/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 0.1340 - val_loss: 6.5317\n",
            "Epoch 273/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.1312 - val_loss: 6.5491\n",
            "Epoch 274/500\n",
            "3168/3168 [==============================] - 3s 843us/step - loss: 0.1287 - val_loss: 6.5402\n",
            "Epoch 275/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.1263 - val_loss: 6.5438\n",
            "Epoch 276/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.1239 - val_loss: 6.5358\n",
            "Epoch 277/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 0.1213 - val_loss: 6.5513\n",
            "Epoch 278/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.1205 - val_loss: 6.5247\n",
            "Epoch 279/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.1223 - val_loss: 6.5429\n",
            "Epoch 280/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.1253 - val_loss: 6.5399\n",
            "Epoch 281/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.1280 - val_loss: 6.5676\n",
            "Epoch 282/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.1292 - val_loss: 6.5344\n",
            "Epoch 283/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.1280 - val_loss: 6.5158\n",
            "Epoch 284/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.1257 - val_loss: 6.5874\n",
            "Epoch 285/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.1263 - val_loss: 6.5477\n",
            "Epoch 286/500\n",
            "3168/3168 [==============================] - 3s 873us/step - loss: 0.1276 - val_loss: 6.5448\n",
            "Epoch 287/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.1471 - val_loss: 6.6365\n",
            "Epoch 288/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.2283 - val_loss: 6.6841\n",
            "Epoch 289/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 0.4320 - val_loss: 6.6989\n",
            "Epoch 290/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.3368 - val_loss: 6.4892\n",
            "Epoch 291/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.2222 - val_loss: 6.5316\n",
            "Epoch 292/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 0.1636 - val_loss: 6.5137\n",
            "Epoch 293/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.1425 - val_loss: 6.4984\n",
            "Epoch 294/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.1261 - val_loss: 6.5098\n",
            "Epoch 295/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 0.1180 - val_loss: 6.5127\n",
            "Epoch 296/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.1127 - val_loss: 6.5443\n",
            "Epoch 297/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.1084 - val_loss: 6.5375\n",
            "Epoch 298/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.1054 - val_loss: 6.5465\n",
            "Epoch 299/500\n",
            "3168/3168 [==============================] - 3s 891us/step - loss: 0.1037 - val_loss: 6.5457\n",
            "Epoch 300/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 0.1013 - val_loss: 6.5519\n",
            "Epoch 301/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.1062 - val_loss: 6.5428\n",
            "Epoch 302/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.1178 - val_loss: 6.5459\n",
            "Epoch 303/500\n",
            "3168/3168 [==============================] - 3s 847us/step - loss: 0.1091 - val_loss: 6.5515\n",
            "Epoch 304/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.1066 - val_loss: 6.5513\n",
            "Epoch 305/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.1083 - val_loss: 6.5735\n",
            "Epoch 306/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 0.1055 - val_loss: 6.5724\n",
            "Epoch 307/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 0.1040 - val_loss: 6.5562\n",
            "Epoch 308/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 0.1001 - val_loss: 6.5649\n",
            "Epoch 309/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0990 - val_loss: 6.5664\n",
            "Epoch 310/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.0951 - val_loss: 6.5729\n",
            "Epoch 311/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 0.0926 - val_loss: 6.5648\n",
            "Epoch 312/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.0906 - val_loss: 6.5545\n",
            "Epoch 313/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.0891 - val_loss: 6.5571\n",
            "Epoch 314/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 0.0876 - val_loss: 6.5671\n",
            "Epoch 315/500\n",
            "3168/3168 [==============================] - 3s 875us/step - loss: 0.0861 - val_loss: 6.5636\n",
            "Epoch 316/500\n",
            "3168/3168 [==============================] - 3s 843us/step - loss: 0.0852 - val_loss: 6.5640\n",
            "Epoch 317/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 0.0843 - val_loss: 6.5641\n",
            "Epoch 318/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.0829 - val_loss: 6.5684\n",
            "Epoch 319/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.0819 - val_loss: 6.5709\n",
            "Epoch 320/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0811 - val_loss: 6.5662\n",
            "Epoch 321/500\n",
            "3168/3168 [==============================] - 3s 876us/step - loss: 0.0803 - val_loss: 6.5611\n",
            "Epoch 322/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.0795 - val_loss: 6.5635\n",
            "Epoch 323/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.0787 - val_loss: 6.5588\n",
            "Epoch 324/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.0780 - val_loss: 6.5665\n",
            "Epoch 325/500\n",
            "3168/3168 [==============================] - 3s 847us/step - loss: 0.0773 - val_loss: 6.5616\n",
            "Epoch 326/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 0.0765 - val_loss: 6.5713\n",
            "Epoch 327/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.0759 - val_loss: 6.5677\n",
            "Epoch 328/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.0751 - val_loss: 6.5718\n",
            "Epoch 329/500\n",
            "3168/3168 [==============================] - 3s 868us/step - loss: 0.0745 - val_loss: 6.5748\n",
            "Epoch 330/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 0.0739 - val_loss: 6.5653\n",
            "Epoch 331/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0734 - val_loss: 6.5763\n",
            "Epoch 332/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.0728 - val_loss: 6.5801\n",
            "Epoch 333/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.0723 - val_loss: 6.5772\n",
            "Epoch 334/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0716 - val_loss: 6.5870\n",
            "Epoch 335/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0711 - val_loss: 6.5793\n",
            "Epoch 336/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.0707 - val_loss: 6.5858\n",
            "Epoch 337/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0702 - val_loss: 6.5823\n",
            "Epoch 338/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 0.0696 - val_loss: 6.5896\n",
            "Epoch 339/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 0.0691 - val_loss: 6.5808\n",
            "Epoch 340/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0690 - val_loss: 6.5753\n",
            "Epoch 341/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.0694 - val_loss: 6.5912\n",
            "Epoch 342/500\n",
            "3168/3168 [==============================] - 3s 872us/step - loss: 0.0714 - val_loss: 6.5974\n",
            "Epoch 343/500\n",
            "3168/3168 [==============================] - 3s 878us/step - loss: 0.0730 - val_loss: 6.5981\n",
            "Epoch 344/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.0773 - val_loss: 6.5949\n",
            "Epoch 345/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.0826 - val_loss: 6.5892\n",
            "Epoch 346/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.0868 - val_loss: 6.6119\n",
            "Epoch 347/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.0855 - val_loss: 6.5971\n",
            "Epoch 348/500\n",
            "3168/3168 [==============================] - 3s 848us/step - loss: 0.0833 - val_loss: 6.6028\n",
            "Epoch 349/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 0.0814 - val_loss: 6.5985\n",
            "Epoch 350/500\n",
            "3168/3168 [==============================] - 3s 847us/step - loss: 0.0812 - val_loss: 6.5700\n",
            "Epoch 351/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0814 - val_loss: 6.5554\n",
            "Epoch 352/500\n",
            "3168/3168 [==============================] - 3s 877us/step - loss: 0.0793 - val_loss: 6.6268\n",
            "Epoch 353/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 0.0776 - val_loss: 6.5429\n",
            "Epoch 354/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.0755 - val_loss: 6.5499\n",
            "Epoch 355/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 0.0745 - val_loss: 6.5842\n",
            "Epoch 356/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.0735 - val_loss: 6.6044\n",
            "Epoch 357/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.0726 - val_loss: 6.5776\n",
            "Epoch 358/500\n",
            "3168/3168 [==============================] - 3s 844us/step - loss: 0.0718 - val_loss: 6.5992\n",
            "Epoch 359/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.0711 - val_loss: 6.6004\n",
            "Epoch 360/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 0.0707 - val_loss: 6.6106\n",
            "Epoch 361/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 0.0702 - val_loss: 6.6045\n",
            "Epoch 362/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0699 - val_loss: 6.6206\n",
            "Epoch 363/500\n",
            "3168/3168 [==============================] - 3s 844us/step - loss: 0.0700 - val_loss: 6.6121\n",
            "Epoch 364/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 0.0705 - val_loss: 6.5955\n",
            "Epoch 365/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 0.0734 - val_loss: 6.6253\n",
            "Epoch 366/500\n",
            "3168/3168 [==============================] - 3s 840us/step - loss: 0.0777 - val_loss: 6.6273\n",
            "Epoch 367/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.0871 - val_loss: 6.6340\n",
            "Epoch 368/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 0.0994 - val_loss: 6.6167\n",
            "Epoch 369/500\n",
            "3168/3168 [==============================] - 3s 866us/step - loss: 0.1264 - val_loss: 6.5882\n",
            "Epoch 370/500\n",
            "3168/3168 [==============================] - 3s 847us/step - loss: 0.1415 - val_loss: 6.5330\n",
            "Epoch 371/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.1529 - val_loss: 6.6015\n",
            "Epoch 372/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.1537 - val_loss: 6.6028\n",
            "Epoch 373/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.1761 - val_loss: 6.6064\n",
            "Epoch 374/500\n",
            "3168/3168 [==============================] - 3s 865us/step - loss: 0.2762 - val_loss: 6.6425\n",
            "Epoch 375/500\n",
            "3168/3168 [==============================] - 3s 849us/step - loss: 0.3070 - val_loss: 6.6983\n",
            "Epoch 376/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.2310 - val_loss: 6.5877\n",
            "Epoch 377/500\n",
            "3168/3168 [==============================] - 3s 848us/step - loss: 0.1765 - val_loss: 6.5639\n",
            "Epoch 378/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.1405 - val_loss: 6.5357\n",
            "Epoch 379/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.1182 - val_loss: 6.5267\n",
            "Epoch 380/500\n",
            "3168/3168 [==============================] - 3s 842us/step - loss: 0.1064 - val_loss: 6.5540\n",
            "Epoch 381/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0979 - val_loss: 6.5375\n",
            "Epoch 382/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.0924 - val_loss: 6.5471\n",
            "Epoch 383/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.0890 - val_loss: 6.5627\n",
            "Epoch 384/500\n",
            "3168/3168 [==============================] - 3s 844us/step - loss: 0.0866 - val_loss: 6.5741\n",
            "Epoch 385/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0848 - val_loss: 6.5623\n",
            "Epoch 386/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.0822 - val_loss: 6.5854\n",
            "Epoch 387/500\n",
            "3168/3168 [==============================] - 3s 875us/step - loss: 0.0807 - val_loss: 6.5845\n",
            "Epoch 388/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.0796 - val_loss: 6.5959\n",
            "Epoch 389/500\n",
            "3168/3168 [==============================] - 3s 844us/step - loss: 0.0787 - val_loss: 6.5934\n",
            "Epoch 390/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0780 - val_loss: 6.5952\n",
            "Epoch 391/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 0.0774 - val_loss: 6.6012\n",
            "Epoch 392/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0769 - val_loss: 6.6043\n",
            "Epoch 393/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.0764 - val_loss: 6.6161\n",
            "Epoch 394/500\n",
            "3168/3168 [==============================] - 3s 847us/step - loss: 0.0759 - val_loss: 6.6149\n",
            "Epoch 395/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.0756 - val_loss: 6.6147\n",
            "Epoch 396/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.0751 - val_loss: 6.6150\n",
            "Epoch 397/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 0.0749 - val_loss: 6.6246\n",
            "Epoch 398/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.0748 - val_loss: 6.6197\n",
            "Epoch 399/500\n",
            "3168/3168 [==============================] - 3s 844us/step - loss: 0.0750 - val_loss: 6.6278\n",
            "Epoch 400/500\n",
            "3168/3168 [==============================] - 3s 842us/step - loss: 0.0743 - val_loss: 6.6312\n",
            "Epoch 401/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.0738 - val_loss: 6.6344\n",
            "Epoch 402/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.0733 - val_loss: 6.6411\n",
            "Epoch 403/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0730 - val_loss: 6.6328\n",
            "Epoch 404/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0727 - val_loss: 6.6428\n",
            "Epoch 405/500\n",
            "3168/3168 [==============================] - 3s 843us/step - loss: 0.0727 - val_loss: 6.6443\n",
            "Epoch 406/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.0729 - val_loss: 6.6453\n",
            "Epoch 407/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0729 - val_loss: 6.6475\n",
            "Epoch 408/500\n",
            "3168/3168 [==============================] - 3s 842us/step - loss: 0.0732 - val_loss: 6.6548\n",
            "Epoch 409/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.0742 - val_loss: 6.6457\n",
            "Epoch 410/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0756 - val_loss: 6.6555\n",
            "Epoch 411/500\n",
            "3168/3168 [==============================] - 3s 855us/step - loss: 0.0757 - val_loss: 6.6744\n",
            "Epoch 412/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.0754 - val_loss: 6.6701\n",
            "Epoch 413/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.0738 - val_loss: 6.6619\n",
            "Epoch 414/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0728 - val_loss: 6.6640\n",
            "Epoch 415/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 0.0725 - val_loss: 6.6670\n",
            "Epoch 416/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.0719 - val_loss: 6.6750\n",
            "Epoch 417/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 0.0718 - val_loss: 6.6734\n",
            "Epoch 418/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.0714 - val_loss: 6.6685\n",
            "Epoch 419/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.0712 - val_loss: 6.6833\n",
            "Epoch 420/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.0710 - val_loss: 6.6770\n",
            "Epoch 421/500\n",
            "3168/3168 [==============================] - 3s 838us/step - loss: 0.0710 - val_loss: 6.6669\n",
            "Epoch 422/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0706 - val_loss: 6.6743\n",
            "Epoch 423/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 0.0706 - val_loss: 6.6771\n",
            "Epoch 424/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.0704 - val_loss: 6.6788\n",
            "Epoch 425/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.0702 - val_loss: 6.6819\n",
            "Epoch 426/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 0.0701 - val_loss: 6.6770\n",
            "Epoch 427/500\n",
            "3168/3168 [==============================] - 3s 877us/step - loss: 0.0699 - val_loss: 6.6857\n",
            "Epoch 428/500\n",
            "3168/3168 [==============================] - 3s 873us/step - loss: 0.0699 - val_loss: 6.6833\n",
            "Epoch 429/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0698 - val_loss: 6.6871\n",
            "Epoch 430/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.0697 - val_loss: 6.6931\n",
            "Epoch 431/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0696 - val_loss: 6.6881\n",
            "Epoch 432/500\n",
            "3168/3168 [==============================] - 3s 867us/step - loss: 0.0695 - val_loss: 6.6885\n",
            "Epoch 433/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.0693 - val_loss: 6.6900\n",
            "Epoch 434/500\n",
            "3168/3168 [==============================] - 3s 847us/step - loss: 0.0693 - val_loss: 6.6923\n",
            "Epoch 435/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.0691 - val_loss: 6.6954\n",
            "Epoch 436/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0690 - val_loss: 6.7045\n",
            "Epoch 437/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0690 - val_loss: 6.7011\n",
            "Epoch 438/500\n",
            "3168/3168 [==============================] - 3s 864us/step - loss: 0.0689 - val_loss: 6.7063\n",
            "Epoch 439/500\n",
            "3168/3168 [==============================] - 3s 859us/step - loss: 0.0687 - val_loss: 6.7030\n",
            "Epoch 440/500\n",
            "3168/3168 [==============================] - 3s 840us/step - loss: 0.0688 - val_loss: 6.6981\n",
            "Epoch 441/500\n",
            "3168/3168 [==============================] - 3s 869us/step - loss: 0.0687 - val_loss: 6.7038\n",
            "Epoch 442/500\n",
            "3168/3168 [==============================] - 3s 843us/step - loss: 0.0685 - val_loss: 6.7068\n",
            "Epoch 443/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.0685 - val_loss: 6.7091\n",
            "Epoch 444/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0684 - val_loss: 6.7086\n",
            "Epoch 445/500\n",
            "3168/3168 [==============================] - 3s 862us/step - loss: 0.0682 - val_loss: 6.7071\n",
            "Epoch 446/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.0683 - val_loss: 6.7165\n",
            "Epoch 447/500\n",
            "3168/3168 [==============================] - 3s 875us/step - loss: 0.0681 - val_loss: 6.7186\n",
            "Epoch 448/500\n",
            "3168/3168 [==============================] - 3s 846us/step - loss: 0.0681 - val_loss: 6.7166\n",
            "Epoch 449/500\n",
            "3168/3168 [==============================] - 3s 863us/step - loss: 0.0680 - val_loss: 6.7132\n",
            "Epoch 450/500\n",
            "3168/3168 [==============================] - 3s 870us/step - loss: 0.0679 - val_loss: 6.7180\n",
            "Epoch 451/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.0678 - val_loss: 6.7154\n",
            "Epoch 452/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.0679 - val_loss: 6.7211\n",
            "Epoch 453/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0677 - val_loss: 6.7239\n",
            "Epoch 454/500\n",
            "3168/3168 [==============================] - 3s 878us/step - loss: 0.0677 - val_loss: 6.7279\n",
            "Epoch 455/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 0.0675 - val_loss: 6.7269\n",
            "Epoch 456/500\n",
            "3168/3168 [==============================] - 3s 861us/step - loss: 0.0675 - val_loss: 6.7242\n",
            "Epoch 457/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0674 - val_loss: 6.7312\n",
            "Epoch 458/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.0674 - val_loss: 6.7268\n",
            "Epoch 459/500\n",
            "3168/3168 [==============================] - 3s 839us/step - loss: 0.0672 - val_loss: 6.7303\n",
            "Epoch 460/500\n",
            "3168/3168 [==============================] - 3s 849us/step - loss: 0.0671 - val_loss: 6.7303\n",
            "Epoch 461/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0672 - val_loss: 6.7339\n",
            "Epoch 462/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.0670 - val_loss: 6.7271\n",
            "Epoch 463/500\n",
            "3168/3168 [==============================] - 3s 888us/step - loss: 0.0671 - val_loss: 6.7306\n",
            "Epoch 464/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.0669 - val_loss: 6.7418\n",
            "Epoch 465/500\n",
            "3168/3168 [==============================] - 3s 844us/step - loss: 0.0669 - val_loss: 6.7333\n",
            "Epoch 466/500\n",
            "3168/3168 [==============================] - 3s 847us/step - loss: 0.0668 - val_loss: 6.7367\n",
            "Epoch 467/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0668 - val_loss: 6.7497\n",
            "Epoch 468/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0668 - val_loss: 6.7436\n",
            "Epoch 469/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 0.0667 - val_loss: 6.7499\n",
            "Epoch 470/500\n",
            "3168/3168 [==============================] - 3s 848us/step - loss: 0.0666 - val_loss: 6.7426\n",
            "Epoch 471/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.0666 - val_loss: 6.7504\n",
            "Epoch 472/500\n",
            "3168/3168 [==============================] - 3s 849us/step - loss: 0.0666 - val_loss: 6.7417\n",
            "Epoch 473/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.0665 - val_loss: 6.7463\n",
            "Epoch 474/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0665 - val_loss: 6.7398\n",
            "Epoch 475/500\n",
            "3168/3168 [==============================] - 3s 851us/step - loss: 0.0665 - val_loss: 6.7527\n",
            "Epoch 476/500\n",
            "3168/3168 [==============================] - 3s 878us/step - loss: 0.0665 - val_loss: 6.7503\n",
            "Epoch 477/500\n",
            "3168/3168 [==============================] - 3s 849us/step - loss: 0.0662 - val_loss: 6.7522\n",
            "Epoch 478/500\n",
            "3168/3168 [==============================] - 3s 843us/step - loss: 0.0662 - val_loss: 6.7688\n",
            "Epoch 479/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.0666 - val_loss: 6.7709\n",
            "Epoch 480/500\n",
            "3168/3168 [==============================] - 3s 856us/step - loss: 0.0672 - val_loss: 6.7595\n",
            "Epoch 481/500\n",
            "3168/3168 [==============================] - 3s 838us/step - loss: 0.0732 - val_loss: 6.7517\n",
            "Epoch 482/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.0762 - val_loss: 6.7241\n",
            "Epoch 483/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.1028 - val_loss: 6.8693\n",
            "Epoch 484/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.3001 - val_loss: 6.7853\n",
            "Epoch 485/500\n",
            "3168/3168 [==============================] - 3s 875us/step - loss: 0.4549 - val_loss: 6.8196\n",
            "Epoch 486/500\n",
            "3168/3168 [==============================] - 3s 857us/step - loss: 0.3321 - val_loss: 6.7503\n",
            "Epoch 487/500\n",
            "3168/3168 [==============================] - 3s 847us/step - loss: 0.2335 - val_loss: 6.6875\n",
            "Epoch 488/500\n",
            "3168/3168 [==============================] - 3s 848us/step - loss: 0.1664 - val_loss: 6.7268\n",
            "Epoch 489/500\n",
            "3168/3168 [==============================] - 3s 860us/step - loss: 0.1375 - val_loss: 6.6764\n",
            "Epoch 490/500\n",
            "3168/3168 [==============================] - 3s 858us/step - loss: 0.1156 - val_loss: 6.6921\n",
            "Epoch 491/500\n",
            "3168/3168 [==============================] - 3s 841us/step - loss: 0.1021 - val_loss: 6.6673\n",
            "Epoch 492/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 0.0948 - val_loss: 6.6975\n",
            "Epoch 493/500\n",
            "3168/3168 [==============================] - 3s 853us/step - loss: 0.0900 - val_loss: 6.7007\n",
            "Epoch 494/500\n",
            "3168/3168 [==============================] - 3s 854us/step - loss: 0.0869 - val_loss: 6.7199\n",
            "Epoch 495/500\n",
            "3168/3168 [==============================] - 3s 845us/step - loss: 0.0843 - val_loss: 6.7253\n",
            "Epoch 496/500\n",
            "3168/3168 [==============================] - 3s 839us/step - loss: 0.0829 - val_loss: 6.7246\n",
            "Epoch 497/500\n",
            "3168/3168 [==============================] - 3s 850us/step - loss: 0.0818 - val_loss: 6.7275\n",
            "Epoch 498/500\n",
            "3168/3168 [==============================] - 3s 871us/step - loss: 0.0810 - val_loss: 6.7304\n",
            "Epoch 499/500\n",
            "3168/3168 [==============================] - 3s 849us/step - loss: 0.0806 - val_loss: 6.7290\n",
            "Epoch 500/500\n",
            "3168/3168 [==============================] - 3s 852us/step - loss: 0.0802 - val_loss: 6.7345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W4L0r-6Cwwg",
        "colab_type": "code",
        "outputId": "3f8a2019-d807-437b-a37c-b3b87c5c4e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# model = load_model('model.h5')\n",
        "preds = model.predict_classes(testX)\n",
        "a = np.random.choice(testX.shape[0])\n",
        "preds[a], testY[a]\n",
        "convert(trn_tokenizer, preds[a])\n",
        "print()\n",
        "convert(rus_tokenizer, testX[a])\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49 ---->  d l' 'a' _ \n",
            "13049 ---->  s t a n o'1 v' 'ix ts ts ax' \n",
            "49 ---->  d l' 'a' _ \n",
            "2796 ---->  k ax r a b l' 'i'1 \n",
            "5020 ---->  r a z u'1 ch' 'ix v ax' l' 'ix' \n",
            "5021 ---->  s' t' 'i' h' 'i1 %% %% \n",
            "5022 ---->  r' 'ix' p' 'i' t' 'i1 r ax v ax' l' 'ix \n",
            "13053 ---->  d a r o1 zh e \n",
            "\n",
            "10051 ----> лейтмотивом\n",
            "1617 ----> сессии\n",
            "756 ----> станут\n",
            "340 ----> проблемы\n",
            "2694 ----> общей\n",
            "262 ----> безопасности\n",
            "3 ----> и\n",
            "2041 ----> демократии\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1c443a86-96a3-4676-b6ad-4e905b6f8999",
        "id": "nTzqkY5ZtGdS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == n:\n",
        "          return word\n",
        "  return None\n",
        "  \n",
        "a = np.random.choice(trn_vocab_size)\n",
        "t = []\n",
        "for e in testX[0]:\n",
        "  s = get_word(e, rus_tokenizer)\n",
        "  if s: t.append(s)\n",
        "print( ''.join(t) )\n",
        "\n",
        "t = []\n",
        "for e in testY[0]:\n",
        "  s = get_word(e, trn_tokenizer)\n",
        "  if s: t.append(s)\n",
        "print( ' '.join(t) )"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "космический аппарат передал сенсационные снимки\n",
            "k a s m' 'i'1 ch' 'ix s k' 'ix' j' # 'a p a r a1 t # p' 'ix' r' 'i d a1 l # s' 'ix n s ax ts y o1 n n ax' jax' # s' n' 'i1 m k' 'i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsT8avD8C-wh",
        "colab_type": "code",
        "outputId": "f180d1eb-63d1-4a51-f4a6-abc2ace5b7b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds_text = []\n",
        "prd = preds[:3]\n",
        "for i in prd:\n",
        "  temp = []\n",
        "  for j in range(len(i)):\n",
        "    t = get_word(i[j], trn_tokenizer)\n",
        "    if j > 0:\n",
        "      if (t == get_word(i[j-1], trn_tokenizer)) or (t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t+'#')\n",
        "    else:\n",
        "      if(t == None):\n",
        "            temp.append('')\n",
        "      else:\n",
        "              temp.append(t+'#') \n",
        "\n",
        "  preds_text.append(''.join(temp))\n",
        "\n",
        "def unpack(tensor):\n",
        "  t = []\n",
        "  for e in tensor:\n",
        "    s = get_word(e, rus_tokenizer)\n",
        "    if s: t.append(s)\n",
        "  return ' '.join(t)\n",
        "\n",
        "preds_text\n",
        "# print( list(zip((unpack(e) for e in testX[:3]), preds_text)))\n",
        "# pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "# pred_df.sample(15)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '', '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgbiEv2w_W_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "1638cba9-a527-4222-fc0b-406b479ea41a"
      },
      "source": [
        "!wget http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-25 05:57:35--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.24.108.196, 104.24.109.196, 2606:4700:3037::6818:6cc4, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.24.108.196|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7747747 (7.4M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "\rdeu-eng.zip           0%[                    ]       0  --.-KB/s               \rdeu-eng.zip          23%[===>                ]   1.72M  8.29MB/s               \rdeu-eng.zip         100%[===================>]   7.39M  24.6MB/s    in 0.3s    \n",
            "\n",
            "2020-02-25 05:57:36 (24.6 MB/s) - ‘deu-eng.zip’ saved [7747747/7747747]\n",
            "\n",
            "Archive:  deu-eng.zip\n",
            "  inflating: deu.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IPzaHenLOFc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "987212cd-e6ee-417b-8e26-a94abfb9c286"
      },
      "source": [
        "import re\n",
        "import string\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "  \n",
        "# split text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents\n",
        "\n",
        "# download data from http://www.manythings.org/anki/deu-eng.zip\n",
        "data = read_text(\"deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)\n",
        "\n",
        "# use first 50,000 English-German sentence pairs\n",
        "deu_eng = deu_eng[:50000,:]\n",
        "\n",
        "# Text Pre-processing\n",
        "# Remove punctuation\n",
        "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
        "deu_eng[:,2] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        " \n",
        "# convert to lowercase\n",
        "for i in range(len(deu_eng)):\n",
        "    deu_eng[i,0] = deu_eng[i,0].lower()\n",
        "    deu_eng[i,2] = '<sos> ' + deu_eng[i,1].lower()\n",
        "    deu_eng[i,1] = '<sos> ' + deu_eng[i,1].lower()\n",
        "\n",
        "# Convert text to sequence \n",
        "# empty lists\n",
        "eng_l = []\n",
        "deu_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "    eng_l.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "    deu_l.append(len(i.split()))\n",
        "\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "    tokenizer = Tokenizer( )\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "  \n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "\n",
        "# prepare Deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines, sos=False, eos=False):\n",
        "\n",
        "    # integer encode sequences\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq\n",
        "  \n",
        "# model building\n",
        "# split data \n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "trainX_ = encode_sequences(deu_tokenizer, deu_length, train[:, 2])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
        "testX_ = encode_sequences(deu_tokenizer, deu_length, test[:, 2])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 6345\n",
            "Deutch Vocabulary Size: 10502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3a6K0uQ-m4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "da27d5f0-f270-4fb3-8613-b0b32b1a37fe"
      },
      "source": [
        "# build NMT model\n",
        "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(units))\n",
        "    model.add(RepeatVector(out_timesteps))\n",
        "    model.add(LSTM(units, return_sequences=True))\n",
        "    model.add(Dense(out_vocab, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def build_model1(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "    # encoder\n",
        "    encoder_inputs = Input(shape=(None,))\n",
        "    encoder_embedding = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)\n",
        "    encoder = encoder_embedding(encoder_inputs)\n",
        "    _, state_h, state_c = LSTM(units, return_state=True)(encoder)\n",
        "    # decoder\n",
        "    decoder_inputs = Input(shape=(None,))\n",
        "    decoder_embedding = Embedding(out_vocab, units, input_length=out_timesteps, mask_zero=True)\n",
        "    decoder = decoder_embedding(decoder_inputs)\n",
        "    decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder, initial_state=(state_h, state_c))\n",
        "    decoder_dense = Dense(out_vocab, activation='softmax')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    return Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "  \n",
        "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# train model\n",
        "filename = 'model.h1.24_jan_19'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
        "          epochs=30, batch_size=512, \n",
        "          validation_split = 0.2,\n",
        "          callbacks=[checkpoint], verbose=1)\n",
        "\n",
        "# history = model.fit([trainX, trainX_], trainY.reshape(*trainY.shape, 1), \n",
        "#           epochs=30, batch_size=512, \n",
        "#           validation_data=([testX,testX_], testY.reshape(*testY.shape, 1)),\n",
        "#           # validation_split = 0.2,\n",
        "#           callbacks=[checkpoint], verbose=1)\n",
        "\n",
        "# plot validation loss vs training loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()\n",
        "\n",
        "# load saved model\n",
        "model = load_model('model.h1.24_jan_19')\n",
        "\n",
        "# make predictions\n",
        "# preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))\n",
        "\n",
        "def get_word(n, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == n:\n",
        "            return word\n",
        "    return None\n",
        "  \n",
        "# convert predictions into text (English)\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "    temp = []\n",
        "    for j in range(len(i)):\n",
        "        t = get_word(i[j], eng_tokenizer)\n",
        "        if j > 0:\n",
        "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)\n",
        "             \n",
        "        else:\n",
        "            if(t == None):\n",
        "                temp.append('')\n",
        "            else:\n",
        "                temp.append(t)            \n",
        "        \n",
        "    preds_text.append(' '.join(temp))\n",
        "    \n",
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "\n",
        "# display results\n",
        "pred_df.sample(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 32000 samples, validate on 8000 samples\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "32000/32000 [==============================] - 321s 10ms/step - loss: 3.4687 - val_loss: 2.9632\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.96317, saving model to model.h1.24_jan_19\n",
            "Epoch 2/30\n",
            "31232/32000 [============================>.] - ETA: 6s - loss: 2.8659 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1n7bwnZwIIc7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "c86f507f-9757-4e58-8d27-7aa531649dd2"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "decoder_state_input_h = Input(shape=(LSTM_NODES,))\n",
        "decoder_state_input_c = Input(shape=(LSTM_NODES,))\n",
        "decoder_states_inputs = (decoder_state_input_h, decoder_state_input_c)\n",
        "decoder_inputs_single = Input(shape=(1,))\n",
        "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
        "decoder_states = [h, c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_single] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "preds = decoder_model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-1c95a98ebf77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoder_state_input_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_NODES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_NODES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdecoder_states_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder_state_input_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_state_input_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdecoder_inputs_single\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder_inputs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTpyAZQWGZ3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}