{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workinprogress.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andygoosh/samsung/blob/master/workinprogress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_Mg8x0gASeS",
        "colab_type": "code",
        "outputId": "b4b278ec-2da4-4ece-f09c-923c25602f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from google.colab import files, drive\n",
        "from collections import defaultdict\n",
        "\n",
        "import re\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array, argmax, random, take\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, GRU, Embedding, RepeatVector, TimeDistributed\n",
        "from keras.layers import Bidirectional as Bi\n",
        "from keras import optimizers\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "RS = 77\n",
        "rn.seed(RS)\n",
        "# tf.random.set_seed(RS)\n",
        "np.random.seed(RS)\n",
        "np.random.RandomState(RS)\n",
        "\n",
        "gpath = Path('/content/gdrive')\n",
        "drive.mount(str(gpath))\n",
        "data_file = gpath / 'My Drive/Samsung' / 'transcriptions'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a0fc22d70461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mgpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'My Drive/Samsung'\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'transcriptions'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mauth_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\nEnter your authorization code:\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrote_to_fifo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         )\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KByWPn1DbpS",
        "colab_type": "text"
      },
      "source": [
        "#### Let's look at the data in given file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNLzpBAGDgAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with data_file.open() as f:  \n",
        "    print(list(f.readline()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmvmq8NkDzzF",
        "colab_type": "text"
      },
      "source": [
        "#### Notice that:\n",
        "1. russian sentence is separated from transcript with '\\t'\n",
        "2. the begining and the end of transcript part are marked by '%%'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9inDldeBBz8",
        "colab_type": "text"
      },
      "source": [
        "### Let's read the data and split it into rus and trans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze9lEBcZB4A_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn = []\n",
        "with data_file.open() as f:  \n",
        "  for line in f: \n",
        "    rus, trn = line.split('\\t')\n",
        "    rus_trn.append([rus.strip(), trn.strip()])\n",
        "\n",
        "print(f'Number of sentences in corpus: {len(rus_trn)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjFV-XOHBN8o",
        "colab_type": "text"
      },
      "source": [
        "##### Let's look at some sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaCdAWXfBS-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a,b,c = np.random.choice(len(rus_trn), 3)\n",
        "\n",
        "print(rus_trn[a][0])\n",
        "print(rus_trn[a][1])\n",
        "print(rus_trn[b][0])\n",
        "print(rus_trn[b][1])\n",
        "print(rus_trn[c][0])\n",
        "print(rus_trn[c][1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrtVAlRiBYLc",
        "colab_type": "text"
      },
      "source": [
        "#### Notice that words in russian sentence are separated by space while words in transcript are separated:\n",
        "1. by '#' in general case\n",
        "2. by '_' in case of preposition\n",
        "3. by '%% %%' in case of punctuation signs (dash, coma, etc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYfq4rSFVs6",
        "colab_type": "text"
      },
      "source": [
        "#### Let's see if we have dupliates in corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoPm3mehFfiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = array(rus_trn)\n",
        "seen_rus = Counter(a[:,0])\n",
        "seen_trn = Counter(a[:,1])\n",
        "\n",
        "print(f'Unique rus sentences: {len(seen_rus)} out of {len(rus_trn)}')\n",
        "print(f'Unique trans sentences: {len(seen_trn)} out of {len(rus_trn)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c_7KSzLGs7v",
        "colab_type": "text"
      },
      "source": [
        "#### We have a lot of duplicates! Only 3131 unique sentenses out of 50K in corpus. Please also note that some russian sentences are transcribed into different transcriptions (will look into that later on)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjR7uun4HeJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lens = [each[1] for each in seen_rus.items()]\n",
        "unq = np.unique(lens)\n",
        "qty = [lens.count(each) for each in unq]\n",
        "pd.DataFrame(qty, index=unq).plot.bar(title = 'Rus sentence repeat times', legend=False);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2bIKlVbIAxi",
        "colab_type": "text"
      },
      "source": [
        "#### Most duplicated sentences repeat 12 times, max up to 203 times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCi44KJRImmG",
        "colab_type": "text"
      },
      "source": [
        "### Let's read that data while splitting the tokens. We'll count the tokens in each sentence. If the count in rus and trans is different, we'll record it as anomaly\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fznr-0SJ9nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_l, trn_l, anomaly = [], [], []\n",
        "seen = defaultdict(list)\n",
        "chars = Counter()\n",
        "\n",
        "for j,i in enumerate(rus_trn):\n",
        "  rus = i[0].split()\n",
        "  trn = re.split('#|_|%% %%',i[1])\n",
        "\n",
        "  if i[0] not in seen:\n",
        "    if abs(len(rus) - len(trn)) != 0: \n",
        "      anomaly.append(j)\n",
        "      # print(j, ' ', i[0])\n",
        "      # print( i[1])\n",
        "    \n",
        "    else:\n",
        "      rus_l.append(len(rus))\n",
        "      trn_l.append(len(trn))\n",
        "\n",
        "    chars += Counter(i[0])\n",
        "\n",
        "  seen[i[0]].append(j)\n",
        "\n",
        "print(f'Anomalies: {len(anomaly)}')\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "pd.DataFrame({'Number of words in Rus sentence':rus_l}).hist(ax=ax1, bins = 30);\n",
        "pd.DataFrame({'Number of words in Trns sentence':trn_l}).hist(ax=ax2, bins = 30);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxTcio_yKSbh",
        "colab_type": "text"
      },
      "source": [
        "#### So we have 49 anomalies out of 3131 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV_cJH70KuSg",
        "colab_type": "text"
      },
      "source": [
        "### Let's see if we need to clean the data. First let's take a look at rus corpus alphabet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyV9anvZn187",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = sorted(chars.items())\n",
        "pd.DataFrame(s, index=(e[0]+' ' for e in s)).plot.bar(figsize=(18,4), rot=0, title = 'Char frequencies', legend=False)\n",
        "print(f'Number of times \"-\" used: {chars[\"-\"]}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83TZ18z9K8Qy",
        "colab_type": "text"
      },
      "source": [
        "#### Looks good! We neither have punctuations nor capital letters. The only case to check is '-' letter which is used 159 times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqkK_LRKLb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seen = set()\n",
        "seen_dash = set()\n",
        "dash_words = []\n",
        "for rus,trn in rus_trn:\n",
        "  if rus not in seen:\n",
        "    if '-' in rus:\n",
        "      ru_words = rus.split()\n",
        "      for each in ru_words:\n",
        "        if '-' in each:\n",
        "          if each not in seen_dash:\n",
        "            seen_dash.add(each)\n",
        "            dash_words.append(each)\n",
        "  else:\n",
        "    seen.add(rus)\n",
        "  \n",
        "print(dash_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VHh6JUCPGlb",
        "colab_type": "text"
      },
      "source": [
        "#### Okey, words with dash look fine, we'll consider dash as a normal letter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xAjHqyjh3x",
        "colab_type": "text"
      },
      "source": [
        "### So in order to implement autocoder for transcript we'd need to keep special symbools in transcript such as \"\\_\" and \"%% %%\". So let's create a dictionary of sentences rus <-> trans. So let's recreate the dictionary so \"\\_\" and \"%% %%\" are marked with '#' and remove begin and end markers. Plus to that let's get rid of duplicates.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7827cG05JrMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install transliterate\n",
        "# from transliterate import translit\n",
        "# translit('длавды дылпадыал лыдап', 'ru', reversed=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBgF1l93y39m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seen_rus = set()\n",
        "seen_trn = set()\n",
        "\n",
        "res, trn_l, rus_l = [], [], []\n",
        "for rus, trn in rus_trn:\n",
        "\n",
        "  if rus not in seen_rus or trn not in seen_trn:\n",
        "      seen_rus.add(rus)\n",
        "      seen_trn.add(trn)\n",
        "\n",
        "      # trn = trn.replace('%% %%', '%% %% #').replace('_', '_ #')[3:-3]\n",
        "      # rus_l.append( len(rus))\n",
        "      # trn_l.append( len(trn.split(' ')))\n",
        "      # res.append([rus, trn.split(' ')])\n",
        "\n",
        "      trn = trn.replace('%% %%', '%% %% #').replace('_', '_ #')[2:-2]\n",
        "      rus_l.append( len(rus.split()))\n",
        "      trn_l.append( len(trn.split('#')))\n",
        "      # res.append([rus.split(), ['<sos>'] + trn.split('#'), ['<sos>'] + trn.split('#')])\n",
        "      res.append([rus.split(), trn.split('#')])\n",
        "\n",
        "rus_trn_new = array(res)\n",
        "trn_length = max(trn_l)\n",
        "rus_length = max(rus_l)\n",
        "print(f'max len rus: {rus_length}, max len trn: {trn_length}')\n",
        "fig, (ax1, ax2) = plt.subplots(nrows=1,ncols=2,figsize=(12,4))\n",
        "pd.DataFrame({'Number of words in Rus sentence':rus_l}).hist(ax=ax1, bins = 30);\n",
        "pd.DataFrame({'Number of words in Trns sentence':trn_l}).hist(ax=ax2, bins = 30);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID8a8MoaKtVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn_new[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbQ82gWtkTDx",
        "colab_type": "text"
      },
      "source": [
        "### Text to Sequence Conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt-GGIUNlkCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenization(lines, split=' ', char_level=False):\n",
        "  tokenizer = Tokenizer(filters='', lower=False, split=split, char_level=char_level)\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  seq = pad_sequences(sequences=seq, maxlen=length, padding='post')\n",
        "  return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "670W8CC1llqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_tokenizer = tokenization(rus_trn_new[:, 0], split='',  char_level=True)\n",
        "rus_vocab_size = len(rus_tokenizer.word_index) + 1\n",
        "trn_tokenizer = tokenization(rus_trn_new[:, 1], split=' ', char_level=False)\n",
        "trn_vocab_size = len(trn_tokenizer.word_index) + 1\n",
        "print(f'Rus Vocabulary Size: {rus_vocab_size}')\n",
        "print(f'Trns Vocabulary Size: {trn_vocab_size}')\n",
        "a = np.random.choice(rus_vocab_size)\n",
        "print(trn_tokenizer.index_word[a])\n",
        "print(rus_tokenizer.index_word[a])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgJdhdjvqRA9",
        "colab_type": "text"
      },
      "source": [
        "### Model Building\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaO755OI17PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rus_trn_new.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XcT-AUGqa5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test = train_test_split(rus_trn_new, test_size=0.01, random_state = RS)\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(rus_tokenizer, rus_length, train[:, 0])\n",
        "# trainX_ = encode_sequences(trn_tokenizer, trn_length, train[:, 2])\n",
        "trainY = encode_sequences(trn_tokenizer, trn_length, train[:, 1])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(rus_tokenizer, rus_length, test[:, 0])\n",
        "# testX_ = encode_sequences(trn_tokenizer, trn_length, test[:, 2])\n",
        "testY = encode_sequences(trn_tokenizer, trn_length, test[:, 1])\n",
        "\n",
        "trainX.shape, testX.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGzWdS9cips_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print(f'{t} ----> {lang.index_word[t]}')\n",
        "\n",
        "a = np.random.choice(len(trainX))\n",
        "convert(rus_tokenizer, trainX[a])\n",
        "print ()\n",
        "convert(trn_tokenizer, trainY[a])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5FSiYilVXvv",
        "colab_type": "text"
      },
      "source": [
        "#### FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8eeBkKPCPse",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "rus_model = FastText(size=20)\n",
        "trn_model = FastText(size=20)\n",
        "\n",
        "rus_model.build_vocab(sentences=rus_trn_new[:, 0])\n",
        "trn_model.build_vocab(sentences=rus_trn_new[:, 1])\n",
        "\n",
        "total_rus = rus_model.corpus_total_words\n",
        "total_trn = trn_model.corpus_total_words\n",
        "print(total_rus, total_trn)\n",
        "\n",
        "rus_model.train(sentences=rus_trn_new[:,0], total_examples = rus_model.corpus_count, epochs=rus_model.iter)\n",
        "trn_model.train(sentences=rus_trn_new[:,1], total_examples = trn_model.corpus_count, epochs=trn_model.iter)\n",
        "\n",
        "print(rus_model.wv['и'])\n",
        "print(trn_model.wv[\" i _ \"])\n",
        "\n",
        "rus_model.wv.similar_by_vector(trn_model.wv[\" i _\"])\n",
        "# trn_model.wv.similar_by_vector(rus_model.wv['группа'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EovHsdWV0JJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trn_model.most_similar(' d' 'ix p u t a1 t ')\n",
        "# rus_model.most_similar('депутат')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8zTiADVilf",
        "colab_type": "text"
      },
      "source": [
        "### Moving on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a9-aKMvp3OF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY[17]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwOy43R15_yn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BS = 128\n",
        "EPOCHES = 500\n",
        "\n",
        "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  input = x = Input(shape=(None,))\n",
        "  x = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)(x)\n",
        "  x, h, c = LSTM(units, return_state=True)(x)\n",
        "  x = RepeatVector(out_timesteps)(x)\n",
        "  x = LSTM(units, return_sequences=True)(x, initial_state = (h, c))\n",
        "  output = TimeDistributed(Dense(out_vocab, activation='softmax'))(x)\n",
        "  inference_model = model = Model(input, output)\n",
        "  return model, inference_model\n",
        "\n",
        "def define_model1(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "   # encoder\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  encoder_embedding = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)\n",
        "  encoder = encoder_embedding(encoder_inputs)\n",
        "  _, state_h, state_c = LSTM(units, return_state=True)(encoder)\n",
        "  encoder_states = [state_h, state_c]\n",
        "  # decoder\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  decoder_embedding = Embedding(out_vocab, units, input_length=out_timesteps, mask_zero=True)\n",
        "  decoder = decoder_embedding(decoder_inputs)\n",
        "  decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
        "  decoder_dense = Dense(out_vocab, activation='softmax')\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  #inference model\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "  decoder_state_input_h = Input(shape=(units,))\n",
        "  decoder_state_input_c = Input(shape=(units,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_inputs_single = Input(shape=(None,))\n",
        "  decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
        "  decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n",
        "  decoder_states = [h, c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  inference_model = Model(\n",
        "      [decoder_inputs_single] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states\n",
        "  )\n",
        "  return model, inference_model\n",
        "\n",
        "model, model_inf = define_model(rus_vocab_size, trn_vocab_size, rus_length, trn_length, 128)\n",
        "model.summary()\n",
        "\n",
        "# optimizer = optimizers.RMSprop(lr=0.001)\n",
        "optimizer=optimizers.Adam()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
        "\n",
        "monitor = 'val_loss'\n",
        "mode = 'min'\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor=monitor, verbose=1, save_best_only=True, mode=mode)\n",
        "early_stop = EarlyStopping( patience=5, monitor=monitor, mode=mode)\n",
        "\n",
        "# train model\n",
        "history = model.fit(trainX, trainY.reshape(*trainY.shape, 1),\n",
        "# history = model.fit([trainX,trainY], trainY.reshape(*trainY.shape, 1),\n",
        "                    validation_data=(testX, testY.reshape(*testY.shape, 1)),\n",
        "                    # validation_data=([testX,trainY], testY.reshape(*testY.shape, 1)),\n",
        "                    epochs=EPOCHES, batch_size=BS, callbacks=[], \n",
        "                    verbose=1)\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W4L0r-6Cwwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = load_model('model.h5')\n",
        "preds = model_inf.predict(testX)\n",
        "preds1 = np.argmax(preds,axis=2)\n",
        "a = np.random.choice(testY.shape[0])\n",
        "preds[a], testY[a]\n",
        "convert(trn_tokenizer, preds1[a])\n",
        "print()\n",
        "convert(rus_tokenizer, testX[a])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nTzqkY5ZtGdS",
        "colab": {}
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == n:\n",
        "          return word\n",
        "  return None\n",
        "  \n",
        "a = np.random.choice(trn_vocab_size)\n",
        "t = []\n",
        "for e in testX[0]:\n",
        "  s = get_word(e, rus_tokenizer)\n",
        "  if s: t.append(s)\n",
        "print( ' '.join(t) )\n",
        "\n",
        "t = []\n",
        "for e in testY[0]:\n",
        "  s = get_word(e, trn_tokenizer)\n",
        "  if s: t.append(s)\n",
        "print( '#'.join(t) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsT8avD8C-wh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_text = []\n",
        "prd = preds[:3]\n",
        "for i in prd:\n",
        "  temp = []\n",
        "  for j in range(len(i)):\n",
        "    t = get_word(i[j], trn_tokenizer)\n",
        "    if j > 0:\n",
        "      if (t == get_word(i[j-1], trn_tokenizer)) or (t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t+'#')\n",
        "    else:\n",
        "      if(t == None):\n",
        "            temp.append('')\n",
        "      else:\n",
        "              temp.append(t+'#') \n",
        "\n",
        "  preds_text.append(''.join(temp))\n",
        "\n",
        "def unpack(tensor):\n",
        "  t = []\n",
        "  for e in tensor:\n",
        "    s = get_word(e, rus_tokenizer)\n",
        "    if s: t.append(s)\n",
        "  return ' '.join(t)\n",
        "\n",
        "preds_text\n",
        "# print( list(zip((unpack(e) for e in testX[:3]), preds_text)))\n",
        "# pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "# pred_df.sample(15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuVg8UxovdB8",
        "colab_type": "text"
      },
      "source": [
        "# German"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgbiEv2w_W_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://www.manythings.org/anki/deu-eng.zip\n",
        "!unzip deu-eng.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IPzaHenLOFc",
        "colab_type": "code",
        "outputId": "1db3325b-7a0a-4e8c-c1c4-42fd0610c8b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "import string\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "  \n",
        "# split text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents\n",
        "\n",
        "# download data from http://www.manythings.org/anki/deu-eng.zip\n",
        "data = read_text(\"deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)\n",
        "\n",
        "# use first 50,000 English-German sentence pairs\n",
        "deu_eng = deu_eng[:50000,:]\n",
        "\n",
        "# Text Pre-processing\n",
        "# Remove punctuation\n",
        "deu_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,0]]\n",
        "deu_eng[:,2] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        "deu_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in deu_eng[:,1]]\n",
        " \n",
        "# convert to lowercase\n",
        "for i in range(len(deu_eng)):\n",
        "    deu_eng[i,1] = deu_eng[i,1].lower() # input (DE)\n",
        "    deu_eng[i,0] = deu_eng[i,0].lower() # target output (EN)\n",
        "\n",
        "    deu_eng[i,2] = '<sos> ' + deu_eng[i,0] # target input (EN)\n",
        "    deu_eng[i,0] = deu_eng[i,0] + ' <eos>'# target output (EN)\n",
        "\n",
        "# Convert text to sequence \n",
        "# empty lists\n",
        "eng_l = []\n",
        "deu_l = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "    eng_l.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "    deu_l.append(len(i.split()))\n",
        "\n",
        "# function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "  \n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = tokenization(deu_eng[:, 2])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "\n",
        "# prepare Deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines, sos=False, eos=False):\n",
        "    # integer encode sequences\n",
        "    seq = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "    return seq\n",
        "  \n",
        "# model building\n",
        "# split data \n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(deu_eng, test_size=0.2, random_state = 12)\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "trainX_ = encode_sequences(eng_tokenizer, eng_length, train[:, 2])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
        "testX_ = encode_sequences(eng_tokenizer, eng_length, test[:, 2])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 6346\n",
            "Deutch Vocabulary Size: 10501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ4mYVSezSpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "339b9e7e-ca92-437c-f0e4-1cbc2d318a67"
      },
      "source": [
        "deu_eng[:, 0], deu_eng[:, 1], deu_eng[:, 2]"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['hi <eos>', 'hi <eos>', 'run <eos>', ...,\n",
              "        'he has no specific aim <eos>', 'he has only four pesos <eos>',\n",
              "        'he has stopped smoking <eos>'], dtype='<U537'),\n",
              " array(['hallo', 'grüß gott', 'lauf', ..., 'er hat kein bestimmtes ziel',\n",
              "        'er hat nur vier pesos', 'er hörte mit dem rauchen auf'],\n",
              "       dtype='<U537'),\n",
              " array(['<sos> hi', '<sos> hi', '<sos> run', ...,\n",
              "        '<sos> he has no specific aim', '<sos> he has only four pesos',\n",
              "        '<sos> he has stopped smoking'], dtype='<U537'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0TnfxYFST65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "aa076ae7-9ed5-4f0b-e709-d65023d04940"
      },
      "source": [
        "trainX[0], trainX_[0], trainY[0]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 53,   3, 325, 487,   0,   0,   0,   0], dtype=int32),\n",
              " array([  1,  69,   6,  23, 438,   2,   0,   0], dtype=int32),\n",
              " array([ 69,   6,  23, 438,   2,   0,   0,   0], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3a6K0uQ-m4K",
        "colab_type": "code",
        "outputId": "d13adb10-63b8-4192-ce82-f9ad29ca561c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "EPOCHES = 100\n",
        "BS = 512\n",
        "\n",
        "TF = True\n",
        "\n",
        "# build NMT model\n",
        "def define_model_simple(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  input = x = Input(shape=(None,))\n",
        "  x = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)(x)\n",
        "  x, h, c = LSTM(units, return_state=True)(x)\n",
        "  x = RepeatVector(out_timesteps)(x)\n",
        "  x = LSTM(units, return_sequences=True)(x, initial_state = (h, c))\n",
        "  output = TimeDistributed(Dense(out_vocab, activation='softmax'))(x)\n",
        "  model = Model(input, output)\n",
        "  return model, None, None\n",
        "\n",
        "def define_model_tf(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "   # encoder\n",
        "  encoder_inputs = Input(shape=(None,))\n",
        "  encoder_embedding = Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True)\n",
        "  encoder = encoder_embedding(encoder_inputs)\n",
        "  _, state_h, state_c = LSTM(units, return_state=True)(encoder)\n",
        "  encoder_states = [state_h, state_c]\n",
        "  # decoder\n",
        "  decoder_inputs = Input(shape=(None,))\n",
        "  decoder_embedding = Embedding(out_vocab, units, input_length=out_timesteps, mask_zero=True)\n",
        "  decoder = decoder_embedding(decoder_inputs)\n",
        "  decoder_lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "  decoder_outputs, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
        "  decoder_dense = TimeDistributed(Dense(out_vocab, activation='softmax'))\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "  #inference model\n",
        "  encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "  decoder_state_input_h = Input(shape=(units,))\n",
        "  decoder_state_input_c = Input(shape=(units,))\n",
        "  decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_outputs, h, c = decoder_lstm(decoder, initial_state=decoder_states_inputs)\n",
        "  decoder_states = [h, c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = Model(\n",
        "      [decoder_inputs] + decoder_states_inputs,\n",
        "      [decoder_outputs] + decoder_states)\n",
        "  \n",
        "  return model, encoder_model, decoder_model\n",
        "\n",
        "define_model = define_model_tf if TF else define_model_simple\n",
        "\n",
        "model, encoder_model, decoder_model = define_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "model.summary()\n",
        "  \n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "monitor = 'val_loss'\n",
        "mode = 'min'\n",
        "early_stop = EarlyStopping( patience=5, monitor=monitor, mode=mode, restore_best_weights=True)\n",
        "\n",
        "if TF:\n",
        "  data = [trainX, trainX_], trainY.reshape(*trainY.shape, 1)\n",
        "  validation_data = [testX, testX_], testY.reshape(*testY.shape, 1)\n",
        "else:\n",
        "  data = trainX, trainY.reshape(*trainY.shape, 1)\n",
        "  validation_data = testX, testY.reshape(*testY.shape, 1)\n",
        "\n",
        "history = model.fit(*data, \n",
        "          epochs=EPOCHES, batch_size=BS, \n",
        "          validation_data=validation_data,\n",
        "          callbacks=[early_stop], verbose=1)\n",
        "\n",
        "# plot validation loss vs training loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_75\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_118 (InputLayer)          (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_119 (InputLayer)          (None, None)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_52 (Embedding)        (None, 8, 512)       5376512     input_118[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_53 (Embedding)        (None, 8, 512)       3249152     input_119[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_55 (LSTM)                  [(None, 512), (None, 2099200     embedding_52[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_56 (LSTM)                  [(None, 8, 512), (No 2099200     embedding_53[0][0]               \n",
            "                                                                 lstm_55[0][1]                    \n",
            "                                                                 lstm_55[0][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_8 (TimeDistrib (None, 8, 6346)      3255498     lstm_56[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 16,079,562\n",
            "Trainable params: 16,079,562\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 21s 513us/step - loss: 5.5421 - val_loss: 4.6005\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 9s 225us/step - loss: 4.3145 - val_loss: 4.0305\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 9s 225us/step - loss: 3.7382 - val_loss: 3.5655\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 9s 223us/step - loss: 3.2509 - val_loss: 3.1487\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 9s 224us/step - loss: 2.8078 - val_loss: 2.7904\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 9s 227us/step - loss: 2.4254 - val_loss: 2.5113\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 9s 225us/step - loss: 2.0961 - val_loss: 2.2816\n",
            "Epoch 8/100\n",
            " 3584/40000 [=>............................] - ETA: 7s - loss: 1.8695"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tR7csfYuPx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "24767a69-48b7-414a-fd38-c7f65ee30006"
      },
      "source": [
        "trainX[10], trainX_[10], trainY[10]"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([22, 75,  4, 76,  0,  0,  0,  0], dtype=int32),\n",
              " array([ 1, 27, 68, 38, 12,  2,  0,  0], dtype=int32),\n",
              " array([27, 68, 38, 12,  2,  0,  0,  0], dtype=int32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTpyAZQWGZ3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "09fae0d6-0656-409a-a965-ba548b44bdf7"
      },
      "source": [
        "def translate_sentence(input_seq):\n",
        "  states_value = encoder_model.predict(input_seq, use_multiprocessing=True)\n",
        "  target_seq = np.array([0] * eng_length)\n",
        "  target_seq[0] = eng_tokenizer.word_index['<sos>']\n",
        "  eos = eng_tokenizer.word_index['<eos>']\n",
        "  output_sentence = []\n",
        "\n",
        "  for _ in range(eng_length):\n",
        "      output_tokens, h, c = decoder_model.predict([target_seq.reshape(1, -1)] + states_value, use_multiprocessing=True)\n",
        "      idx = np.argmax(output_tokens[0, 0, :])\n",
        "\n",
        "      if eos == idx:\n",
        "          break\n",
        "\n",
        "      if idx > 0:\n",
        "          word = eng_tokenizer.index_word[idx]\n",
        "          output_sentence.append(word)\n",
        "\n",
        "      target_seq[0] = idx\n",
        "      states_value = [h, c]\n",
        "\n",
        "  return ' '.join(output_sentence)\n",
        "\n",
        "a = np.random.choice(len(testX), 15)\n",
        "x = testX[a]\n",
        "\n",
        "if TF:\n",
        "  translation = []\n",
        "  for i in a:\n",
        "    input_seq = testX[i]\n",
        "    translation.append( translate_sentence(input_seq) )\n",
        "else:\n",
        "  pred_ohe = model.predict(x, use_multiprocessing=True)\n",
        "  pred = np.argmax(pred_ohe, axis=2)\n",
        "  translation = eng_tokenizer.sequences_to_texts(pred)\n",
        "\n",
        "pred_df = pd.DataFrame({'actual' : eng_tokenizer.sequences_to_texts(testY[a]), 'predicted' : translation})\n",
        "pred_df"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>make it stop &lt;eos&gt;</td>\n",
              "      <td>do you go out</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the signal was red &lt;eos&gt;</td>\n",
              "      <td>that is fun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tom makes me better &lt;eos&gt;</td>\n",
              "      <td>tom is fun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>he made her his wife &lt;eos&gt;</td>\n",
              "      <td>he is it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>you should rest &lt;eos&gt;</td>\n",
              "      <td>do you recycle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>i knew youd go there &lt;eos&gt;</td>\n",
              "      <td>i am it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>did you bring it &lt;eos&gt;</td>\n",
              "      <td>have it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>im a perfectionist &lt;eos&gt;</td>\n",
              "      <td>i am it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>tom walked home &lt;eos&gt;</td>\n",
              "      <td>tom is fun</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>i love my friends &lt;eos&gt;</td>\n",
              "      <td>i am it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>my study is upstairs &lt;eos&gt;</td>\n",
              "      <td>my way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>im uninsured &lt;eos&gt;</td>\n",
              "      <td>i am it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>im losing &lt;eos&gt;</td>\n",
              "      <td>i am it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>wheres the other one &lt;eos&gt;</td>\n",
              "      <td>take tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>i am off duty today &lt;eos&gt;</td>\n",
              "      <td>today is it</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        actual       predicted\n",
              "0           make it stop <eos>   do you go out\n",
              "1     the signal was red <eos>     that is fun\n",
              "2    tom makes me better <eos>      tom is fun\n",
              "3   he made her his wife <eos>        he is it\n",
              "4        you should rest <eos>  do you recycle\n",
              "5   i knew youd go there <eos>         i am it\n",
              "6       did you bring it <eos>         have it\n",
              "7     im a perfectionist <eos>         i am it\n",
              "8        tom walked home <eos>      tom is fun\n",
              "9      i love my friends <eos>         i am it\n",
              "10  my study is upstairs <eos>          my way\n",
              "11          im uninsured <eos>         i am it\n",
              "12             im losing <eos>         i am it\n",
              "13  wheres the other one <eos>        take tom\n",
              "14   i am off duty today <eos>     today is it"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjP8HJluWDus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}